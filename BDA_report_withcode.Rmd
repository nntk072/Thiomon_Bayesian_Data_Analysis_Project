---
title: "Thiomon Data Analysis Report"
author: "Long Nguyen and Amaanat Ali"
output:
  html_document:
    toc: yes
    toc_depth: '5'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 5
bibliography: references.bib
csl: vancouver.csl
---


```{r include=FALSE}
suppressWarnings({
# setwd("/notebooks/bda2024/Bayes_project/Bayes_project")
setwd("C:/Users/nguye/OneDrive - Aalto University/University/First year/Project/Bayes_project")
SEED = 42
if(!require(nnet)) {
  install.packages("nnet")
  library(nnet)
}
if (!require(rstan)) {
    install.packages("rstan")
    library(rstan)
}

if (!require(loo)) {
    install.packages("loo")
    library(loo)
}

if (!require(gridExtra)) {
    install.packages("gridExtra")
    library(gridExtra)
}

if (!require(grid)) {
    install.packages("grid")
    library(grid)
}
if (!require(rmarkdown)) {
    install.packages("rmarkdown")
    library(rmarkdown)
}

if (!require(tidybayes)) {
    install.packages("tidybayes")
    library(tidybayes)
}

if (!require(brms)) {
    install.packages("brms")
    library(brms)
}

if (!require(metadat)) {
  install.packages("metadat")
  library(metadat)
}

if(!require(cmdstanr)){
    install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
    library(cmdstanr)
}

cmdstan_installed <- function(){
  res <- try(out <- cmdstanr::cmdstan_path(), silent = TRUE)
  !inherits(res, "try-error")
}

if(!cmdstan_installed()){
    install_cmdstan()
}

if(!require(ggplot2)){
    install.packages("ggplot2")
    library(ggplot2)
}

ggplot2::theme_set(theme_minimal(base_size = 14))
if(!require(bayesplot)){
    install.packages("bayesplot")
    library(bayesplot)
}

if(!require(posterior)){
    install.packages("posterior")
    library(posterior)
}

if (!require(priorsense)) {
  install.packages("priorsense")
  library(priorsense)
}

if (!require(tibble)) {
  install.packages("tibble")
  library(tibble)
}

if (!require(RColorBrewer)) {
  install.packages("RColorBrewer")
  library(RColorBrewer)
}

if (!require(dplyr)) {
  install.packages("dplyr")
  library(dplyr)
}

if (!require(pROC)) {
  install.packages("pROC")
  library(pROC)
}

if (!require(caret)) {
  install.packages("caret")
  library(caret)
}

if (!require(tinytable)) {
  install.packages("tinytable")
  library(tinytable)
}

if (!require(corrplot)) {
  install.packages("corrplot")
  library(corrplot)
}

if (!require(arm)) {
  install.packages("arm")
  library(arm)
}

if (!require(projpred)) {
  install.packages("projpred")
  library(projpred)
}
if(!require(e1071)) {
  install.packages("e1071")
  library(e1071)
}
if(!require(caret)) {
  install.packages("caret")
  library(caret)
}
options(tinytable_format_num_fmt = "significant_cell", tinytable_format_digits = 2, tinytable_tt_digits=2)
})
```



\newpage

(This is an example of using reference) This study explores the use of machine learning algorithms for predicting clinical outcomes with thiopurines [@10.1093/ecco-jcc/jjx014]. you can change the sections/subsections/name according to your idea

# 1. Introduction

Thiopurines are widely used immunomodulators for the treatment of ulcerative colitis [UC] and Crohn's disease [CD] in inflammatory bowel disease [IBD] patients. Thiopurines can induce immune suppression, clinical and biological remission, and are steroid-sparing agents.

# 1.1. Motivation

The study uses a data set based on de-identified Laboratory Data on IBD Patients using Thiourines for at least 4 weeks and their eventual Remission/Active Status after at least 12 Weeks of therapy.

# 1.2. Problem Statement

We plan to apply Bayesian data analysis technique to predict the Objective Remission (OR) in optimizing thiopurine therapy for inflammatory bowel disease by applying pooled, separate and hierarchical models. Furthermore, we make a comparison between the 3 models and draw some observations.

# 1.3. Main Modeling Summary

# 2. Data: Thiomon Dataset

A dataset containing laboratory data and outcomes of IBD patients on Thiopurine therapy at the University of Michigan. Data on laboratory values for a complete blood count and chemistry panel at least 4 weeks after start of thiopurine therapy in IBD patients. The University of Michigan Hospital is in Ann Arbor, USA. These data have been anonymized, and time-shifted. Age is reported in days of life.

## 2.1. Data description

Based on the blood parameters, we can categorize them into broad categories like Hematology, Chemistry, Electrolytes, Liver Function, and Inflammation/Other Markers.

**Hematology Parameters (Related to blood cells, hemoglobin, and related components)**

Platelet Count (plt)

Mean Platelet Volume (mpv)

White Blood Cell Count (wbc)

Hemoglobin (hgb)

Hematocrit (hct)

Red Blood Cell Count (rbc)

Mean Corpuscular Volume (MCV) (mcv)

Mean Corpuscular Hemoglobin (MCH) (mch)

Mean Corpuscular Hemoglobin Concentration (MCHC) (mchc)

Red Cell Distribution Width (RDW) (rdw)

Neutrophil Percent (neut_percent)

Lymphocyte Percent (lymph_percent)

Monocyte Percent (mono_percent)

Eosinophil Percent (eos_percent)

Basophil Percent (baso_percent)

**Chemistry Parameters (General body chemistry markers)**

Blood Urea Nitrogen (BUN) (un)

Sodium (sod)

Potassium (pot)

Chloride (chlor)

Bicarbonate (CO2) (co2)

Creatinine (creat)

Glucose (gluc)

Calcium (cal)

Protein (prot)

Albumin (alb)

**Liver Function Parameters (Enzyme and bilirubin levels)**

Aspartate Transaminase (AST) (ast)

Alanine Transaminase (ALT) (alt)

Alkaline Phosphatase (ALK) (alk)

Total Bilirubin (Tbil) (tbil

**Inflammation and Other Markers**

Active Inflammation Despite Thiopurines for \> 12 Weeks (active)

Remission of Inflammation After Thiopurines for \> 12 Weeks (remission)

## 2.2. Data preprocessing

Load the data comprising of approximately 5K rows of blood chemistry with remission and active status (each is 1 bit and complementary to each other).

```{r}
#Loading the data
load("thiomon.rda")
thiomon <- na.omit(thiomon)
```

```{r}
#Process the data set with some additional processing
source_data <- thiomon

#Take the ratio of hgb and hct which describes if the patient is anaemic or not
source_data$hbghctrat = round(thiomon$hgb/thiomon$hct,2)

#Group into 5 groups
source_data$hbghctrat <- cut(source_data$hbghctrat, breaks = seq(min(source_data$hbghctrat), max(source_data$hbghctrat), length.out = 6), labels = c(1,2,3,4,5))

#Convert this to a character label
source_data$hbghctrat <- as.character(source_data$hbghctrat)

source_data$hgbratnum <- cut(source_data$hgb, breaks = seq(min(source_data$hgb), max(source_data$hgb), length.out = 6), labels = c(1,2,3,4,5))

source_data$hgbrat <- as.character(source_data$hgbratnum)

#Remove invalid entries
source_data <- na.omit(source_data)

# Sample a subset of the 4500 rows to improve computation time
MAX_DATA_SAMPLES <- 4000

#Number of samples for the likelihood
NUM_DATA_SAMPLES <- 2000
train_data_norm <- source_data[sample(MAX_DATA_SAMPLES, NUM_DATA_SAMPLES), ]

#Number of samples for the testing
test_data <- source_data[4001:5000, ]

NUM_DATA_SAMPLES_ALT = 40

temp_var <- source_data[source_data$hgbrat == 1,]
train_data_alt <- temp_var[sample(nrow(temp_var), NUM_DATA_SAMPLES_ALT), ]

temp_var <- source_data[source_data$hgbrat == 2,]
train_data_alt <- rbind(train_data_alt, temp_var[sample(nrow(temp_var), NUM_DATA_SAMPLES_ALT), ])

temp_var <- source_data[source_data$hgbrat == 3,]
train_data_alt <- rbind(train_data_alt, temp_var[sample(nrow(temp_var), NUM_DATA_SAMPLES_ALT), ])

temp_var <- source_data[source_data$hgbrat == 4,]
train_data_alt <- rbind(train_data_alt, temp_var[sample(nrow(temp_var), NUM_DATA_SAMPLES_ALT), ])

temp_var <- source_data[source_data$hgbrat == 5,]
train_data_alt <- rbind(train_data_alt, temp_var[sample(nrow(temp_var), NUM_DATA_SAMPLES_ALT), ])

#Set it to our alternate data set
train_data <- train_data_norm
```

### 2.2.1 Identifying variables for our modelling

There are 30 blood parameters which are identified by the database. Using the MLE algorithm 'lm', we can quickly check that all of these variables together explain 66% of the variability in the data set. Unfortunately, using all of them are computationally inefficient and will take long times to fit a bayesian model. Hence we need to prune the list down.

```{#lm_formula <- remission ~ 0 + days_of_life + wbc + hgb + hct + plt + rbc + mcv + mch + mchc + rdw + mpv + lymph_percent + neut_percent + mono_percent + eos_percent + baso_percent + sod + pot + chlor + co2 + un + creat + gluc + cal + prot + alb + ast + alt + alk + tbil}
#residual sd = 0.44, R-Squared = 0.66

#residual sd = 0.47, R-Squared = 0.60
#lm_formula <- remission ~ 0 + hgb + hct + mchc

#From paper's reco
#residual sd = 0.46, R-Squared = 0.60
lm_formula <- remission ~ 0 + hgb + lymph_percent + hct + neut_percent + plt + alb + 
ast

lm1 <- lm(formula = lm_formula, data = train_data)
display(lm1)

fitg <- stan_glm(formula = lm_formula, data = train_data, refresh=0)
summary(fitg)

fitg0 <- update(fitg, formula = remission ~ 1, QR=FALSE)

(loog0 <- loo(fitg0))

(loog <- loo(fitg, k_threshold=0.7))

loo_compare(loog0, loog)

fitg_cv <- cv_varsel(fitg, method='forward', cv_method='LOO', validate_search=FALSE)

plot(fitg_cv, stats = c('elpd', 'rmse'))

(nsel <- suggest_size(fitg_cv, alpha=0.1))

(vsel <- solution_terms(fitg_cv)[1:nsel])
```

The model with covariates definitely has a better EPLD compared to the base model and hence having covariates in our bayesian model definitely make sense. Going through the exercise, the 'hgb' and 'lymph_percent' seem the dominant variables and they have 58% explanatory power [Ref: <https://users.aalto.fi/~ave/modelselection/mesquite.html>].

```{lm_formula <- remission ~ 0 + hgb + lymph_percent}

lm1 <- lm(formula = lm_formula, data = train_data)
display(lm1)
```

We continue with this choice further in our model fitting and also remember that 'lymph_percent' adds only 1% more explanatory power overall, so we may also decide to drop it.

To allow for slope and intercept to be defined in the separate and hierarchical models, we need to distribute 'hgb' into N classes across the range of values. Furthermore, to ensure that the number of data samples are same in each class (otherwise the likelihood statistic will force a larger variance), we sample each class and pick M samples. Hence the total number of samples for our study become M x N.

# 3. Models

## 3.1. Pooled model

Make a basic pooled model.

```{r warning=FALSE, message=FALSE}
ref_thiomon_formulae_pooled <- bf(remission ~ 1, family = "bernoulli")

thiomon_priors_default_priors_pooled <- get_prior(ref_thiomon_formulae_pooled, data = train_data)

thiomon_set_priors_informative_pooled <- c(
  prior(
    normal(0,1),
    class = "Intercept"
  ) 
)

thiomon_data_model_pooled <- brm(
    formula = ref_thiomon_formulae_pooled,
    prior = thiomon_set_priors_informative_pooled,
    data = train_data,
    save_pars = save_pars(all = TRUE),
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.8,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

There are no divergent transactions reported. Let's check for some model related diagnostics.

### 3.1.1 R-hat and Effective Sample Size (ESS)

First checking the model for any discrepancies in R-hat and ESS.

```{r}
thiomon_data_model_pooled
```

R-hat and effective sample sizes look good. At convergence also R-hat value is 1 means the chains have converged well.

### 3.1.2 Pareto k-hat

```{r}
loo(thiomon_data_model_pooled)
```

There are no Pareto k-hat estimated greater than 0.7 which is what we desire.

### 3.1.3 Choosing the prior

Continuing with checks on prior, justification for the chosen prior is that the default prior 'student_t(3, 0, 2.5)' is almost equivalent to a 'normal(0, 1.5)' but we find that the underlying 'theta' of the distribution is '0.48' which allows us to make a narrower choice.

```{r}
data.frame(theta = plogis(rnorm(n=20000, mean=0, sd=1.0))) |>
  mcmc_hist() +
  xlim(c(0,1)) +
  labs(title='normal(0, 1.0) for Intercept') 
```

It seems it is a correct choice as there is no conflict revealed from the prior diagnostics.

### 3.1.4 Posterior prior sensitivity

```{r}
thiomon_data_model_pooled |>
  powerscale_plot_dens(help_text=FALSE)
```

The values themselves are also very small.

```{r}
thiomon_data_model_pooled |>
  powerscale_sensitivity() |> tt()
```

Posterior predictive checks seem fine as well. Density overlay seems to fit well.

```{r}
pp_check(thiomon_data_model_pooled, type = 'dens_overlay', ndraws = 200)
```

### 3.1.5 Posterior predictive checking

LOO-PIT check seems fine as well.

```{r}
pp_check(thiomon_data_model_pooled, type = 'loo_pit_qq', ndraws = 4000)
```

LOO-PIT ECDF is within the blue recommendation.

```{r}
pp_check(thiomon_data_model_pooled, type = 'pit_ecdf', ndraws = 4000)
```

### 3.1.6 Model parameters

Value of the population intercept 'alpha' is estimated at 0.14. We exclude the likelihood parameters.

```{r}
as_draws_df(thiomon_data_model_pooled)  |> subset_draws(variable=c('lprior','lp__'), exclude=TRUE) |> summarise_draws() |> tt()
```

## 3.2. Separate model

Try with the separate model. Let's try with the sample one below using the recommended parameters from above section:

```{r warning=FALSE, message=FALSE}
ref_thiomon_formulae_separate <- bf(remission ~ 0 + hgbrat + lymph_percent, family = "bernoulli")

thiomon_priors_default_priors_separate <- get_prior(ref_thiomon_formulae_separate, data = train_data)

(thiomon_set_priors_informative_separate <- c(
  prior(
    normal(-2,5),
    class = "b"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "lymph_percent"
  )
))

thiomon_data_model_separate <- brm(
    formula = ref_thiomon_formulae_separate,
    prior = thiomon_set_priors_informative_separate,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.9,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

### 3.2.1 Checking if the covariates are normally distributed

```{r}
train_data |>
  ggplot(aes(x = hgb)) + xlim(c(0,50)) +
  geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "MCMC Histogram of Haemoglobin", x = "Parameter Value", y = "Frequency")
```

```{r}
train_data |>
  ggplot(aes(x = lymph_percent)) + xlim(c(0,75)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "MCMC Histogram of Haemocrit", x = "Parameter Value", y = "Frequency")
```

Both the parameters demonstrate a normal distribution around their respective means. We can approximate with normal distribution for all the parameters.

We don't use flat priors as they are uninformative and we have lots of data to give us good reference values. Also with flat priors, posterior prior checking tools due to input containing infinite or NA values as there is a constant tail. We notice also that the entire population parameters are assigned the same priors. We want to change it. Also notice that we don't have population level Intercept.

### 3.2.2 Checking prior sensitivity

With our chosen priors (we used the default priors first and then used the posterior of the slopes to judge what we need), we expect to see no prior prior sensitivity and we are right. Power-scaling with cumulative Jensen-Shannon distance diagnostic indicates no prior-data conflict.

```{r}
thiomon_data_model_separate |>
  powerscale_plot_dens(help_text=FALSE)
```

```{r}
thiomon_data_model_separate |>
  powerscale_sensitivity() |> tt()
```

### 3.2.3 R-hat and Effective Sample Size (ESS)

First checking the model for any discrepancies in R-hat and ESS before proceeding any further.

```{r}
thiomon_data_model_separate
```

R-hat and effective sample sizes look good. At convergence also R-hat value is 1 means the chains have converged well.

```{r}
loo(thiomon_data_model_separate)
```

### 3.2.4 Issue with Pareto K-hat and solution

```{r}
loo(thiomon_data_model_separate)
```

In case there are complaints about Pareto k-estimates, we use the following command and refit the model by removing the observations causing the conflict. Here the model was refit 1 times as there was 1 value with value between 0.7 and 1.

```{r}
thiomon_data_model_separate <- add_criterion(thiomon_data_model_separate, criterion='loo', reloo=TRUE)
```

The Pareto k-estimates indicate there are no observations which have k \> 0.7. Posterior predictive checks seem fine as well. Density overlay seems to fit well.

### 3.2.5 Posterior predictive checking

```{r}
pp_check(thiomon_data_model_separate, type = 'dens_overlay', ndraws = 200)
```

LOO-PIT check seems fine as well.

```{r}
pp_check(thiomon_data_model_separate, type = 'loo_pit_qq', ndraws = 4000)
```

LOO-PIT ECDF is within the recommendation.

```{r}
pp_check(thiomon_data_model_separate, type = 'pit_ecdf', ndraws = 4000)
```

### 3.2.6 Model parameters

The slopes for the covariates can be listed using the following

```{r}
as_draws_df(thiomon_data_model_separate)  |> subset_draws(variable=c('lprior','lp__'), exclude=TRUE) |> summarise_draws() |> tt()
```

## 3.3. Hierarchical model

Let's try a hierarchical model. In 'hier_1' there is no global intercept but in 'hier_2' there is.

```{r warning=FALSE, message=FALSE}
ref_thiomon_formulae_hier_1 <- bf(remission ~ 0 + lymph_percent + (1 | hgbrat), family = "bernoulli")

thiomon_priors_default_priors_hier_1 <- get_prior(ref_thiomon_formulae_hier_1, data = train_data)

thiomon_set_priors_informative_hier_1 <- c(
  
  prior(
    normal(2.5,4),
    class = "sd"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "lymph_percent"
  )
)

thiomon_data_model_hier_1 <- brm(
    formula = ref_thiomon_formulae_hier_1,
    prior = thiomon_set_priors_informative_hier_1,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.95,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

Note we have also set an informative prior on the Intercept as flat priors cause problems in loo checking due to Pareto K-hat values.

```{r warning=FALSE, message=FALSE}
ref_thiomon_formulae_hier_2 <- bf(remission ~ 1 + lymph_percent + (1 | hgbrat), family = "bernoulli")

thiomon_priors_default_priors_hier_2 <- get_prior(ref_thiomon_formulae_hier_2, data = train_data)

thiomon_set_priors_informative_hier_2 <- c(
  
  prior(
    normal(-1.5,2),
    class = "Intercept"
  ),
  
  prior(
    normal(1.5,3),
    class = "sd"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "lymph_percent"
  )
)

thiomon_data_model_hier_2 <- brm(
    formula = ref_thiomon_formulae_hier_2,
    prior = thiomon_set_priors_informative_hier_2,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.95,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

### 3.3.1 R-hat and Effective Sample Size (ESS)

R-hat and ESS values look good. Convergence at the end of the run indicates R-hat is 1.0 which means samples are reliable and can be used for inference. Also posterior prior sensitivity is well adjusted.

```{r}
thiomon_data_model_hier_1 |>
  powerscale_sensitivity()
```

The prior sensitivity caused some issues for us. There were a few divergent transactions reported (less than 10) but R-hat values were \<= 1.01 for all the parameters. Also the model reported R-hat values converged at the end of the simulation. When there are those many divergent transactions, we could have fixed them by increasing the 'adapt_delta' but we choose to check the priors and see if they could be tuned. When tuned too tight the diagnostic would reveal a prior-data conflict and when too wide it would complain about likelihood and prior conflict. We had to iterate to get the best balance of reducing the divergent transactions and the prior conflict. The values -1.5 for Intercept and 1.5 for sd are testament to this iterative checking.

The following commands are generic for both the hierarchical model choices. We continue with hier_1 as both the models have almost same EPLD value (negligible difference as expected).

### 3.3.2 Issue with Pareto K-hat and solution

```{r}
loo(thiomon_data_model_hier_1)
```

Since there were no values with problematic K-hat the following is not required.

```{r}
thiomon_data_model_hier_1 <- add_criterion(thiomon_data_model_hier_1, criterion='loo', reloo=TRUE)
```

### 3.3.3 LOO-PIT check

```{r}
pp_check(thiomon_data_model_hier_1, type = 'loo_pit_qq', ndraws = 4000)
```

Try to see if a better hierarchical model fits. The 'sd' prior required a bit of tweaking. Keeping it wider results in 1 divergent transaction which we try to avoid by making it narrow but not quite narrow that we get a prior-data conflict. With this balancing there are no problems we spot.

There is no issue anymore of prior sensitivity.

```{r}
# Predict probabilities on the test data
predicted_probs <- posterior_predict(thiomon_data_model_hier_2, 
                                     newdata = test_data, draws = 4000)

# Calculate the mean predicted probabilities for each test sample
mean_probs <- colMeans(predicted_probs)

# Predict classes based on a threshold of 0.5
predicted_classes <- ifelse(mean_probs >= 0.5, 1, 0)

# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(test_data$remission))

#Convert this to data frame and use tt() for pretty print
print(conf_matrix)
```

## 3.4 Pooled, Separate and Hierarchical model comparision

### 3.4.1 LOO comparing all the 4 models

```{r}
loo_compare(loo(thiomon_data_model_pooled), loo(thiomon_data_model_separate), loo(thiomon_data_model_hier_1), loo(thiomon_data_model_hier_2)) |>
  as.data.frame() |>
  rownames_to_column("model") |>
  dplyr::select(model, elpd_diff, se_diff) |>
  tt()
```

The best performing model is the hierarchical, closely followed by separate with hardly much difference in EPLD (less than 0.5) and finally the pooled (with a EPLD score 18 points below both of the hierarchical and separate models).

### 3.3.2 Model posterior distributions

```{r}
if (!require(patchwork)) {
  install.packages("patchwork")
  library(patchwork)
}

ph1 <- thiomon_data_model_hier_1 |>
  spread_rvars(b_lymph_percent, r_hgbrat[hgbrat,]) |>
  mutate(mean_value = b_lymph_percent + r_hgbrat) |>
  ggplot(aes(xdist= inv_logit_scaled(mean_value), y=hgbrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb Ratio', title='Hierarchical')


ph2 <- thiomon_data_model_hier_2 |>
  spread_rvars(b_Intercept, b_lymph_percent, r_hgbrat[hgbrat,]) |>
  mutate(mean_value = b_Intercept + b_lymph_percent + r_hgbrat) |>
  ggplot(aes(xdist= inv_logit_scaled(mean_value), y=hgbrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb Ratio', title='Hierarchical')

ps_part <- thiomon_data_model_separate |> 
  spread_rvars(b_lymph_percent)

ps_part_fin <- ps_part[rep(1:nrow(ps_part), each = 5), ]

ps <- thiomon_data_model_separate |>
  as_draws_df() |>
  subset_draws(variable = "b_hgbrat", regex = TRUE) |>
  set_variables(paste0('b_hgbrat[', 1:5, ']')) |>
  as_draws_rvars() |>
  spread_rvars(b_hgbrat[hgbrat]) |> cbind(ps_part_fin) |>
  mutate(mean_value = b_hgbrat + b_lymph_percent) |>
  ggplot(aes(xdist=inv_logit_scaled(mean_value), y=hgbrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb Ratio', title='Separate')

pp <- thiomon_data_model_pooled |>
  spread_rvars(b_Intercept) |>
  mutate(mean_value = b_Intercept) |>
  ggplot(aes(xdist=inv_logit_scaled(mean_value), y=0)) +
  stat_halfeye() +
  scale_y_continuous(breaks=NULL) +
  labs(x='Remission factor', y='Across all', title='Pooled model')

(ph1 / ps / pp) * xlim(c(0.0,1.0))

```

The reasons are mostly clear - the separate and hierarchical models perform almost the same but the pooled model does not factor in any covariates and hence only relies on the remission bits as information and hence it has poor predictive power. The close performance of the separate and hierarchical models is due to the fact that the patients segregated by the 'hgbrat' do share some information mutually in the hierarchical model but this is not much which means that the predicted remissions are not widely varying.

### 3.3.3 Bayes and LOO R2 checks

```{r warning=FALSE}
#Pooled model
loo_R2_pooled <- loo_R2(thiomon_data_model_pooled)
Bayes_R2_pooled <- bayes_R2(thiomon_data_model_pooled)

#Separate model
loo_R2_separate <- loo_R2(thiomon_data_model_separate)
Bayes_R2_separate <- bayes_R2(thiomon_data_model_separate)

#Hierarchical model 1
loo_R2_hier_1 <- loo_R2(thiomon_data_model_hier_1)
Bayes_R2_hier_1 <- bayes_R2(thiomon_data_model_hier_1)

#Hierarchical model 2
loo_R2_hier_2 <-loo_R2(thiomon_data_model_hier_2)
Bayes_R2_hier_2 <-bayes_R2(thiomon_data_model_hier_2)
```

The pooled model has negligible values as they don't have any covariates factored in. The separate and hierarchical models do have 14% and 15% explanatory power of the variance of the data. Again this shows the separate and hierarchical models do not differ much here.

The LOO-R2 and Bayes-R2 values both match for all the models indicating that there is no likely over-fitting or under-fitting of the data.

## 3.4. Non-Bayesian model

Due to the accuracy of the model here 
```{r}

model_svm <- svm(remission ~ 0 + days_of_life + wbc + hgb + hct + plt + rbc + mcv + mch + mchc + rdw + mpv + neut_percent + lymph_percent + mono_percent + eos_percent + baso_percent + sod + pot + chlor + co2 + un + creat + gluc + cal + prot + alb + ast + alt + alk + tbil, data = train_data)

```

```{r}

model <- nnet(remission ~ days_of_life + wbc + hgb + hct + plt + rbc + mcv + mch + mchc + rdw + mpv + neut_percent + lymph_percent + mono_percent + eos_percent + baso_percent + sod + pot + chlor + co2 + un + creat + gluc + cal + prot + alb + ast + alt + alk + tbil, data = thiomon, size = 20,    # Number of hidden units 
maxit = 100)  # For classification
```
```{r}
model_glm <- glm(remission ~ 0 + days_of_life + wbc + hgb + hct + plt + rbc + mcv + mch + mchc + rdw + mpv + neut_percent + lymph_percent + mono_percent + eos_percent + baso_percent + sod + pot + chlor + co2 + un + creat + gluc + cal + prot + alb + ast + alt + alk + tbil, data = thiomon)
```
# 4. Evaluation results 

## 4.1. Confusion matrix and Evaluation metrics results
```{r include=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_pooled <- posterior_predict(thiomon_data_model_pooled, newdata = train_data, draws = 4000, allow_new_levels = TRUE)
train_mean_probs_pooled <- colMeans(train_predicted_probs_pooled)

# Find optimal threshold using ROC curve on training data
train_roc_curve_pooled <- roc(train_data$remission, train_mean_probs_pooled)
optimal_threshold_pooled <- coords(train_roc_curve_pooled, "best", ret = "threshold")

# Predict on test data
predicted_probs_pooled <- posterior_predict(thiomon_data_model_pooled, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_pooled <- colMeans(predicted_probs_pooled)

# ROC Curve
roc_curve_pooled <- roc(test_data$remission, mean_probs_pooled)
# plot(roc_curve_pooled, main = "ROC Curve for Separate Model")

# Confusion Matrix
predicted_classes_pooled <- ifelse(mean_probs_pooled >= optimal_threshold_pooled$threshold, 1, 0)
conf_matrix_pooled <- confusionMatrix(as.factor(predicted_classes_pooled), as.factor(test_data$remission))
print(conf_matrix_pooled)

```


```{r include=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_separate <- posterior_predict(thiomon_data_model_separate, newdata = train_data, draws = 4000, allow_new_levels = TRUE)
train_mean_probs_separate <- colMeans(train_predicted_probs_separate)

# Find optimal threshold using ROC curve on training data
train_roc_curve_separate <- roc(train_data$remission, train_mean_probs_separate)
optimal_threshold_separate <- coords(train_roc_curve_separate, "best", ret = "threshold")

# Predict on test data
predicted_probs_separate <- posterior_predict(thiomon_data_model_separate, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_separate <- colMeans(predicted_probs_separate)

# ROC Curve
roc_curve_separate <- roc(test_data$remission, mean_probs_separate)
# plot(roc_curve_separate, main = "ROC Curve for Separate Model")

# Confusion Matrix
predicted_classes_separate <- ifelse(mean_probs_separate >= optimal_threshold_separate$threshold, 1, 0)
conf_matrix_separate <- confusionMatrix(as.factor(predicted_classes_separate), as.factor(test_data$remission))
print(conf_matrix_separate)
```

```{r include=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_hierarchical_1 <- posterior_predict(thiomon_data_model_hier_1, newdata = train_data, draws = 4000, allow_new_levels = TRUE)
train_mean_probs_hierarchical_1 <- colMeans(train_predicted_probs_hierarchical_1)

# Find optimal threshold using ROC curve on training data
train_roc_curve_hierarchical_1 <- roc(train_data$remission, train_mean_probs_hierarchical_1)
optimal_threshold_hierarchical_1 <- coords(train_roc_curve_hierarchical_1, "best", ret = "threshold")

# Predict on test data
predicted_probs_hierarchical_1 <- posterior_predict(thiomon_data_model_hier, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_hierarchical_1 <- colMeans(predicted_probs_hierarchical_1)

# ROC Curve
roc_curve_hierarchical_1 <- roc(test_data$remission, mean_probs_hierarchical_1)
# plot(roc_curve_hierarchical_1, main = "ROC Curve for First Model")

# Confusion Matrix
predicted_classes_hierarchical_1 <- ifelse(mean_probs_hierarchical_1 >= optimal_threshold_hierarchical_1$threshold, 1, 0)
conf_matrix_hierarchical_1 <- confusionMatrix(as.factor(predicted_classes_hierarchical_1), as.factor(test_data$remission))
print(conf_matrix_hierarchical_1)
```



```{r include=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_hierarchical_2 <- posterior_predict(thiomon_data_model_hier_2, newdata = train_data, draws = 4000, allow_new_levels = TRUE)
train_mean_probs_hierarchical_2 <- colMeans(train_predicted_probs_hierarchical_2)

# Find optimal threshold using ROC curve on training data
train_roc_curve_hierarchical_2 <- roc(train_data$remission, train_mean_probs_hierarchical_2)
optimal_threshold_hierarchical_2 <- coords(train_roc_curve_hierarchical_2, "best", ret = "threshold")

# Predict on test data
predicted_probs_hierarchical_2 <- posterior_predict(thiomon_data_model_hier, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_hierarchical_2 <- colMeans(predicted_probs_hierarchical_2)

# ROC Curve
roc_curve_hierarchical_2 <- roc(test_data$remission, mean_probs_hierarchical_2)
# plot(roc_curve_hierarchical_2, main = "ROC Curve for First Model")

# Confusion Matrix
predicted_classes_hierarchical_2 <- ifelse(mean_probs_hierarchical_2 >= optimal_threshold_hierarchical_2$threshold, 1, 0)
conf_matrix_hierarchical_2 <- confusionMatrix(as.factor(predicted_classes_hierarchical_2), as.factor(test_data$remission))
print(conf_matrix_hierarchical_2)
```

```{r include=FALSE}
# Predict probabilities on the training data
pred_probs_svm <- predict(model_svm, train_data, probability = TRUE)

# Generate the ROC curve
train_roc_curve_svm <- roc(train_data$remission, pred_probs_svm)

# Find the optimal threshold
optimal_threshold_svm <- coords(train_roc_curve_svm, "best", ret = "threshold")

# Apply the threshold to the test_data find

# probs in test_data
pred_probs_svm_test <- predict(model_svm, test_data, probability = TRUE)
# Confusion matrix using the optimal_threshold_svm
pred_classes_svm <- ifelse(pred_probs_svm > optimal_threshold_svm$threshold, 1, 0)
conf_matrix_svm <- confusionMatrix(as.factor(pred_classes_svm), as.factor(train_data$remission))
print(conf_matrix_svm)

# Predict probabilities on the test data
pred_probs_svm_test <- predict(model_svm, test_data, probability = TRUE)
roc_curve_svm <- roc(test_data$remission, pred_probs_svm_test)

# Plot the ROC curve
# plot(roc_curve_svm, main = "ROC Curve for SVM Model")
```

```{r include=FALSE}
# Predict probabilities on the training data
pred_probs_glm <- predict(model_glm, train_data, type = "response")

# Generate the ROC curve
train_roc_curve_glm <- roc(train_data$remission, pred_probs_glm)

# Find the optimal threshold
optimal_threshold_glm <- coords(train_roc_curve_glm, "best", ret = "threshold")

# Apply the threshold to the test_data find

# probs in test_data
pred_probs_glm_test <- predict(model_glm, test_data, type = "response") 
# Confusion matrix using the optimal_threshold_glm

pred_classes_glm <- ifelse(pred_probs_glm > optimal_threshold_glm$threshold, 1, 0)
conf_matrix_glm <- confusionMatrix(as.factor(pred_classes_glm), as.factor(train_data$remission))
print(conf_matrix_glm)

# Predict probabilities on the test data
pred_probs_glm_test <- predict(model_glm, test_data, type = "response")
roc_curve_glm <- roc(test_data$remission, pred_probs_glm_test)

# Plot the ROC curve
# plot(roc_curve_glm, main = "ROC Curve for GLM Model")
```

```{r include=FALSE}
# Predict probabilities on the training data
pred_probs_glm <- predict(model_glm, train_data, type = "response")

# Generate the ROC curve
train_roc_curve_glm <- roc(train_data$remission, pred_probs_glm)

# Find the optimal threshold
optimal_threshold_glm <- coords(train_roc_curve_glm, "best", ret = "threshold")

# Apply the threshold to the test_data find

# probs in test_data
pred_probs_glm_test <- predict(model_glm, test_data, type = "response") 
# Confusion matrix using the optimal_threshold_glm

pred_classes_glm <- ifelse(pred_probs_glm > optimal_threshold_glm$threshold, 1, 0)
conf_matrix_glm <- confusionMatrix(as.factor(pred_classes_glm), as.factor(train_data$remission))
print(conf_matrix_glm)

# Predict probabilities on the test data
pred_probs_glm_test <- predict(model_glm, test_data, type = "response")
roc_curve_glm <- roc(test_data$remission, pred_probs_glm_test)

# Plot the ROC curve
# plot(roc_curve_glm, main = "ROC Curve for GLM Model")
```

```{r include=FALSE}
# Predict probabilities on the training data
pred_probs_nn <- predict(model, train_data, type = "raw")

# Generate the ROC curve
train_roc_curve_nn <- roc(train_data$remission, pred_probs_nn)

# Find the optimal threshold
optimal_threshold_nn <- coords(train_roc_curve_nn, "best", ret = "threshold")

# Apply the threshold to the training data
pred_classes_nn <- ifelse(pred_probs_nn > optimal_threshold_nn$threshold, 1, 0)
conf_matrix_nn <- confusionMatrix(as.factor(pred_classes_nn), as.factor(train_data$remission))
print(conf_matrix_nn)

# Predict probabilities on the test data
pred_probs_nn_test <- predict(model, test_data, type = "raw")

# Apply the threshold to the test data
pred_classes_nn_test <- ifelse(pred_probs_nn_test > optimal_threshold_nn$threshold, 1, 0)
conf_matrix_nn_test <- confusionMatrix(as.factor(pred_classes_nn_test), as.factor(test_data$remission))
print(conf_matrix_nn_test)

# Generate the ROC curve
roc_curve_nn <- roc(test_data$remission, pred_probs_nn_test)

# Plot the ROC curve
# plot(roc_curve_nn, main = "ROC Curve for Neural Network Model")
```



## 4.2. ROC curves and AUC values

```{r}
# Function to interpolate and smooth ROC curve with jitter
interpolate_smooth_roc_jitter <- function(roc_curve, n = 100, jitter_amount = 1e-5) {
  roc_smooth <- smooth(roc_curve)
  fpr <- seq(0, 1, length.out = n)
  tpr <- approx(roc_smooth$specificities + runif(length(roc_smooth$specificities), -jitter_amount, jitter_amount), 
                roc_smooth$sensitivities, xout = fpr)$y
  data.frame(fpr = fpr, tpr = tpr)
}

# Calculate and interpolate ROC curves with jitter
roc_pooled_train <- interpolate_smooth_roc_jitter(train_roc_curve_pooled)
roc_pooled_test <- interpolate_smooth_roc_jitter(roc_curve_pooled)
roc_separate_train <- interpolate_smooth_roc_jitter(train_roc_curve_separate)
roc_separate_test <- interpolate_smooth_roc_jitter(roc_curve_separate)
roc_hierarchical_1_train <- interpolate_smooth_roc_jitter(train_roc_curve_hierarchical_1)
roc_hierarchical_1_test <- interpolate_smooth_roc_jitter(roc_curve_hierarchical_1)
roc_hierarchical_2_train <- interpolate_smooth_roc_jitter(train_roc_curve_hierarchical_2)
roc_hierarchical_2_test <- interpolate_smooth_roc_jitter(roc_curve_hierarchical_2)

# glm, svm and nn model too
roc_glm_train <- interpolate_smooth_roc_jitter(train_roc_curve_glm)
roc_glm_test <- interpolate_smooth_roc_jitter(roc_curve_glm)
roc_svm_train <- interpolate_smooth_roc_jitter(train_roc_curve_svm)
roc_svm_test <- interpolate_smooth_roc_jitter(roc_curve_svm)
# roc_nn_train <- interpolate_smooth_roc_jitter(train_roc_curve_nn)
# roc_nn_test <- interpolate_smooth_roc_jitter(roc_curve_nn)

# Calculate AUC values
auc_pooled_train <- auc(train_roc_curve_pooled)
auc_pooled_test <- auc(roc_curve_pooled)
auc_separate_train <- auc(train_roc_curve_separate)
auc_separate_test <- auc(roc_curve_separate)
auc_hierarchical_1_train <- auc(train_roc_curve_hierarchical_1)
auc_hierarchical_1_test <- auc(roc_curve_hierarchical_1)
auc_hierarchical_2_train <- auc(train_roc_curve_hierarchical_2)
auc_hierarchical_2_test <- auc(roc_curve_hierarchical_2)
auc_glm_train <- auc(train_roc_curve_glm)
auc_glm_test <- auc(roc_curve_glm)
auc_svm_train <- auc(train_roc_curve_svm)
auc_svm_test <- auc(roc_curve_svm)
# auc_nn_train <- auc(train_roc_curve_nn)
# auc_nn_test <- auc(roc_curve_nn)

# Combine ROC curves into one data frame

train_roc_data <- rbind(
  data.frame(roc_pooled_train, model = paste("Pool Train (AUC =", round(auc_pooled_train, 4), ")")),
  data.frame(roc_separate_train, model = paste("Separate Train (AUC =", round(auc_separate_train, 4), ")")),
  data.frame(roc_hierarchical_1_train, model = paste("Hierarchical with Global Intercept Train (AUC =", round(auc_hierarchical_1_train, 4), ")")),
  data.frame(roc_hierarchical_2_train, model = paste("Hierarchical without Global Intercept Train (AUC =", round(auc_hierarchical_2_train, 4), ")")),
  data.frame(roc_glm_train, model = paste("GLM Train (AUC =", round(auc_glm_train, 4), ")")),
  data.frame(roc_svm_train, model = paste("SVM Train (AUC =", round(auc_svm_train, 4), ")"))
  #data.frame(roc_nn_train, model = paste("NN Train (AUC =", round(auc_nn_train, 4), ")")),
  #data.frame(roc_nn_test, model = paste("NN Test (AUC =", round(auc_nn_test, 4), ")"))
  )
test_roc_data <- rbind(
  data.frame(roc_pooled_test, model = paste("Pool Test (AUC =", round(auc_pooled_test, 4), ")")),
  data.frame(roc_separate_test, model = paste("Separate Test (AUC =", round(auc_separate_test, 4), ")")),
  data.frame(roc_hierarchical_1_test, model = paste("Hierarchical with Global Intercept Test (AUC =", round(auc_hierarchical_1_test, 4), ")")),
  data.frame(roc_hierarchical_2_test, model = paste("Hierarchical without Global Intercept Test (AUC =", round(auc_hierarchical_2_test, 4), ")")),
  data.frame(roc_glm_test, model = paste("GLM Test (AUC =", round(auc_glm_test, 4), ")")),
  data.frame(roc_svm_test, model = paste("SVM Test (AUC =", round(auc_svm_test, 4), ")"))
  #data.frame(roc_nn_test, model = paste("NN Test (AUC =", round(auc_nn_test, 4), ")"))
  )
# Plot all ROC curves in one plot
ggplot(train_roc_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line() +
  labs(title = "ROC Curves for Different Models", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()
ggplot(test_roc_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line() +
  labs(title = "ROC Curves for Different Models", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()

```

# 5. Discussion of Issues and Potential Improvements

# 6. Conclusion

# 7. Self-Reflection

Regarding to the project, we have learned how to collaborate and help each other in the group. The project has been done with multiple trials and errors, with different parameter fitting. And the final result is the balance trade-off of accuracy and time training. In some cases, some evaluation metrics need the knowledge, leading to our discussion and revision of course content material and providing the oppurtunity for us to improve ourself throughout the professor's project report. Even though the Bayesian classification result cannot be used in real research work and does not have a good accuracy compared to the other research publication, the application of Bayesian and how to evaluate the model can help us for building our fundamental knowledge and inspire our future research path. Moreover, the process of doing the project together can also enhance our skill in teamwork and communication. We really appreciate the oppurtunity from the course orientation to the final project.

# Appendix: Environment setup

```
rm(list = ls()) # Remove the environment variables
setwd("Your working directory here") 
SEED = 42 # Set the seed for reproducibility
if(!require(nnet)) {
  install.packages("nnet")
  library(nnet)
}
if (!require(rstan)) {
    install.packages("rstan")
    library(rstan)
}
if (!require(loo)) {
    install.packages("loo")
    library(loo)
}
if (!require(gridExtra)) {
    install.packages("gridExtra")
    library(gridExtra)
}
if (!require(grid)) {
    install.packages("grid")
    library(grid)
}
if (!require(rmarkdown)) {
    install.packages("rmarkdown")
    library(rmarkdown)
}
if (!require(tidybayes)) {
    install.packages("tidybayes")
    library(tidybayes)
}
if (!require(brms)) {
    install.packages("brms")
    library(brms)
}
if(!require(cmdstanr)){
    install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
    library(cmdstanr)
}
cmdstan_installed <- function(){
  res <- try(out <- cmdstanr::cmdstan_path(), silent = TRUE)
  !inherits(res, "try-error")
}
if(!cmdstan_installed()){
    install_cmdstan()
}
if(!require(ggplot2)){
    install.packages("ggplot2")
    library(ggplot2)
}
ggplot2::theme_set(theme_minimal(base_size = 14))
if(!require(bayesplot)){
    install.packages("bayesplot")
    library(bayesplot)
}
if(!require(posterior)){
    install.packages("posterior")
    library(posterior)
}
if (!require(priorsense)) {
  install.packages("priorsense")
  library(priorsense)
}
if (!require(tibble)) {
  install.packages("tibble")
  library(tibble)
}

if (!require(dplyr)) {
  install.packages("dplyr")
  library(dplyr)
}
if (!require(pROC)) {
  install.packages("pROC")
  library(pROC)
}
if (!require(caret)) {
  install.packages("caret")
  library(caret)
}
if(!require(fitdistrplus)){
    install.packages("fitdistrplus", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
    library(fitdistrplus)
}
if(!require(e1071)) {
  install.packages("e1071")
  library(e1071)
}
if(!require(caret)) {
  install.packages("caret")
  library(caret)
}
options(tinytable_format_num_fmt = "significant_cell", tinytable_format_digits = 2, tinytable_tt_digits=2)
```

# References
