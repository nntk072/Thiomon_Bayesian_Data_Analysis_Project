---
title: "Thiomon Data Analysis Report"
author: "Long Nguyen and Amaanat Ali"
output:
  pdf_document:
    toc: yes
    toc_depth: 5
  html_document:
    toc: yes
    toc_depth: '5'
    df_print: paged
bibliography: references.bib
csl: vancouver.csl
---

```{r include=FALSE}
suppressWarnings({
rm(list = ls())
setwd("/notebooks/bda2024/Bayes_project")
# setwd("C:/Users/nguye/OneDrive - Aalto University/University/First year/Project/Bayes_project")
SEED = 42
if(!require(nnet)) {
  install.packages("nnet")
  library(nnet)
}
if (!require(rstan)) {
    install.packages("rstan")
    library(rstan)
}

if (!require(loo)) {
    install.packages("loo")
    library(loo)
}

if (!require(gridExtra)) {
    install.packages("gridExtra")
    library(gridExtra)
}

if (!require(grid)) {
    install.packages("grid")
    library(grid)
}
if (!require(rmarkdown)) {
    install.packages("rmarkdown")
    library(rmarkdown)
}

if (!require(tidybayes)) {
    install.packages("tidybayes")
    library(tidybayes)
}

if (!require(brms)) {
    install.packages("brms")
    library(brms)
}

if (!require(metadat)) {
  install.packages("metadat")
  library(metadat)
}

if(!require(cmdstanr)){
    install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
    library(cmdstanr)
}

cmdstan_installed <- function(){
  res <- try(out <- cmdstanr::cmdstan_path(), silent = TRUE)
  !inherits(res, "try-error")
}

if(!cmdstan_installed()){
    install_cmdstan()
}

if(!require(ggplot2)){
    install.packages("ggplot2")
    library(ggplot2)
}

ggplot2::theme_set(theme_minimal(base_size = 14))
if(!require(bayesplot)){
    install.packages("bayesplot")
    library(bayesplot)
}

if(!require(posterior)){
    install.packages("posterior")
    library(posterior)
}

if (!require(priorsense)) {
  install.packages("priorsense")
  library(priorsense)
}

if (!require(tibble)) {
  install.packages("tibble")
  library(tibble)
}

if (!require(RColorBrewer)) {
  install.packages("RColorBrewer")
  library(RColorBrewer)
}

if (!require(dplyr)) {
  install.packages("dplyr")
  library(dplyr)
}

if (!require(pROC)) {
  install.packages("pROC")
  library(pROC)
}

if (!require(caret)) {
  install.packages("caret")
  library(caret)
}

if (!require(tinytable)) {
  install.packages("tinytable")
  library(tinytable)
}

if (!require(corrplot)) {
  install.packages("corrplot")
  library(corrplot)
}

if (!require(arm)) {
  install.packages("arm")
  library(arm)
}

if (!require(projpred)) {
  install.packages("projpred")
  library(projpred)
}
if(!require(e1071)) {
  install.packages("e1071")
  library(e1071)
}
if(!require(caret)) {
  install.packages("caret")
  library(caret)
}
if (!require(patchwork)) {
  install.packages("patchwork")
  library(patchwork)
}

options(tinytable_format_num_fmt = "significant_cell", tinytable_format_digits = 2, tinytable_tt_digits=2)
})
```

\newpage

# 1. Introduction

Thiopurines are widely used immunomodulators for the treatment of ulcerative colitis [UC] and Crohn's disease [CD] in inflammatory bowel disease [IBD] patients. Thiopurines can induce immune suppression, clinical and biological remission, and are steroid-sparing agents [@axelrad2016thiopurines].

## 1.1. Motivation

The study uses a data set based on de-identified Laboratory Data on IBD Patients using Thiourines for at least 4 weeks and their eventual Remission/Active Status after at least 12 Weeks of therapy.

## 1.2. Problem Statement

We plan to apply the Bayesian data analysis technique to predict the Objective Remission (OR) in optimizing thiopurine therapy for inflammatory bowel disease by applying pooled, separate, and hierarchical models. Furthermore, we make a comparison between the 3 models and draw some observations.

# 2. Data: Thiomon Dataset

A dataset containing laboratory data and outcomes of IBD patients on Thiopurine therapy at the University of Michigan. Data on laboratory values for a complete blood count and chemistry panel at least 4 weeks after the start of thiopurine therapy in IBD patients [@github_htmlpreview]. The University of Michigan Hospital is in Ann Arbor, USA. These data have been anonymized, and time-shifted. Age is reported in days of life.

## 2.1. Data description

Based on the blood parameters, we can categorize them into broad categories like Hematology, Chemistry, Electrolytes, Liver Function, and Inflammation/Other Markers.

**Hematology Parameters (Related to blood cells, hemoglobin, and related components)**

Platelet Count (plt)

Mean Platelet Volume (mpv)

White Blood Cell Count (wbc)

Hemoglobin (hgb)

Hematocrit (hct)

Red Blood Cell Count (rbc)

Mean Corpuscular Volume (MCV) (mcv)

Mean Corpuscular Hemoglobin (MCH) (mch)

Mean Corpuscular Hemoglobin Concentration (MCHC) (mchc)

Red Cell Distribution Width (RDW) (rdw)

Neutrophil Percent (neut_percent)

Lymphocyte Percent (lymph_percent)

Monocyte Percent (mono_percent)

Eosinophil Percent (eos_percent)

Basophil Percent (baso_percent)

**Chemistry Parameters (General body chemistry markers)**

Blood Urea Nitrogen (BUN) (un)

Sodium (sod)

Potassium (pot)

Chloride (chlor)

Bicarbonate (CO2) (co2)

Creatinine (creat)

Glucose (gluc)

Calcium (cal)

Protein (prot)

Albumin (alb)

**Liver Function Parameters (Enzyme and bilirubin levels)**

Aspartate Transaminase (AST) (ast)

Alanine Transaminase (ALT) (alt)

Alkaline Phosphatase (ALK) (alk)

Total Bilirubin (Tbil) (tbil)

**Inflammation and Other Markers**

Active Inflammation Despite Thiopurines for \> 12 Weeks (active)

Remission of Inflammation After Thiopurines for \> 12 Weeks (remission)

## 2.2. Data preprocessing

Load the data comprising approximately 5K rows of blood chemistry with remission and active status (each is 1 bit and complementary to each other).

```{r include=FALSE}
#Loading the data
load("thiomon.rda")
thiomon <- na.omit(thiomon)

#Process the data set with some additional processing
source_data <- thiomon
```

```{r}
# Take the ratio of hgb and hct which describes if the patient is anaemic or not
source_data$hbghctrat = round(thiomon$hgb/thiomon$hct,2)

# Group into 5 groups
source_data$hbghctrat <- cut(source_data$hbghctrat, breaks = seq(min(source_data$hbghctrat), max(source_data$hbghctrat), length.out = 6), labels = c(1,2,3,4,5))

# Convert this to a character label
source_data$hbghctrat <- as.character(source_data$hbghctrat)
source_data$hgbratnum <- cut(source_data$hgb, breaks = seq(min(source_data$hgb), max(source_data$hgb), length.out = 6), labels = c(1,2,3,4,5))
source_data$hgbrat <- as.character(source_data$hgbratnum)

# Sample a subset of the 4500 rows to improve computation time
MAX_DATA_SAMPLES <- 4500
#Number of samples for the likelihood
NUM_DATA_SAMPLES <- 2000
source_data <- na.omit(source_data)
train_data_norm <- source_data[sample(MAX_DATA_SAMPLES, NUM_DATA_SAMPLES), ]

train_data <- train_data_norm
train_data <- na.omit(train_data)
test_data <- source_data[4501:nrow(source_data), ]
```

## 2.3 Identifying variables for our modeling

There are 30 blood parameters which are identified by the database. Using the MLE algorithm 'lm', we can quickly check that all of these variables together explain 66% of the variability in the data set. Unfortunately, using all of them is computationally inefficient and will take a long time to fit a Bayesian model. Hence we need to prune the list down.

```{r}
#lm_formula <- remission ~ 0 + days_of_life + wbc + hgb + hct + plt + rbc + mcv + mch + #mchc + rdw + mpv + lymph_percent + neut_percent + mono_percent + eos_percent + #baso_percent + sod + pot + chlor + co2 + un + creat + gluc + cal + prot + alb + ast + #alt + alk + tbil}
#residual sd = 0.44, R-Squared = 0.66

#residual sd = 0.47, R-Squared = 0.60
#lm_formula <- remission ~ 0 + hgb + hct + mchc

#From paper's recommendation
#residual sd = 0.46, R-Squared = 0.60
#lm_formula <- remission ~ 0 + hgb + lymph_percent + hct + neut_percent + plt + alb + 
#ast

#lm1 <- lm(formula = lm_formula, data = train_data)
#display(lm1)
#install.packages("rstanarm")
#library(rstanarm)

#fitg <- stan_glm(formula = lm_formula, data = train_data, refresh=0)
#summary(fitg)

#fitg0 <- update(fitg, formula = remission ~ 1, QR=FALSE)

#(loog0 <- loo(fitg0))

#(loog <- loo(fitg, k_threshold=0.7))

#loo_compare(loog0, loog)

#fitg_cv <- cv_varsel(fitg, method='forward', cv_method='LOO', validate_search=FALSE)

#plot(fitg_cv, stats = c('elpd', 'rmse'))

#(nsel <- suggest_size(fitg_cv, alpha=0.1))

#(vsel <- solution_terms(fitg_cv)[1:nsel])
```

The model with covariates definitely has a better EPLD compared to the base model and hence having covariates in our Bayesian model definitely makes sense. Going through the exercise, the 'hgb' and 'lymph_percent' seem the dominant variables and they have 58% explanatory power [@vehtari_collinear_demo].

```{r}
#lm_formula <- remission ~ 0 + hgb + lymph_percent
#lm1 <- lm(formula = lm_formula, data = train_data)
#display(lm1)
```

We continue with this choice further in our model fitting and also remember that 'lymph_percent' adds only 1% more explanatory power overall, so we may also decide to drop it.

To allow for slope and intercept to be defined in the separate and hierarchical models, we need to distribute 'hgb' into N classes across the range of values. Furthermore, to ensure that the number of data samples are same in each class (otherwise the likelihood statistic will force a larger variance), we sample each class and pick M samples. Hence the total number of samples for our study becomes M x N.

# 3. Models

## 3.1. Pooled model

Make a basic pooled model.

```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
ref_thiomon_formulae_pooled <- bf(remission ~ 1, family = "bernoulli")

thiomon_priors_default_priors_pooled <- get_prior(ref_thiomon_formulae_pooled, data = train_data)

thiomon_set_priors_informative_pooled <- c(
  prior(
    normal(0,1),
    class = "Intercept"
  ) 
)

thiomon_data_model_pooled <- brm(
    formula = ref_thiomon_formulae_pooled,
    prior = thiomon_set_priors_informative_pooled,
    data = train_data,
    save_pars = save_pars(all = TRUE),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    control = list(
      adapt_delta = 0.8,
      max_treedepth = 20
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
})})
```

There are no divergent transactions reported. Let's check for some model-related diagnostics.

### 3.1.1 R-hat and Effective Sample Size (ESS)

First checking the model for any discrepancies in R-hat and ESS.

```{r echo=FALSE}
thiomon_data_model_pooled
```

R-hat and effective sample sizes look good. At convergence also R-hat value is 1 means the chains have converged well.

### 3.1.2 Pareto k-hat

```{r echo=FALSE}
loo(thiomon_data_model_pooled)
```

There are no Pareto k-hat estimated greater than 0.7 which is what we desire.

### 3.1.3 Choosing the prior

Continuing with checks on prior, justification for the chosen prior is that the default prior 'student_t(3, 0, 2.5)' is almost equivalent to a 'normal(0, 1.5)' but we find that the underlying 'theta' of the distribution is '0.48' which allows us to make a narrower choice.

```{r echo=FALSE}
data.frame(theta = plogis(rnorm(n=20000, mean=0, sd=1.0))) |>
  mcmc_hist() +
  xlim(c(0,1)) +
  labs(title='normal(0, 1.0) for Intercept') 
```

It seems it is a correct choice as there is no conflict revealed from the prior diagnostics.

### 3.1.4 Posterior prior sensitivity

```{r echo=FALSE}
thiomon_data_model_pooled |>
  powerscale_plot_dens(help_text=FALSE)
```

The values themselves are also very small.

```{r echo=FALSE}
thiomon_data_model_pooled |>
  powerscale_sensitivity() |> tt()
```

Posterior predictive checks seem fine as well. Bars in the plot seem to fit well.

```{r echo=FALSE}
pp_check(thiomon_data_model_pooled, type = 'bars', ndraws = 200)
```

### 3.1.5 Posterior Predictive Checking

LOO-PIT check seems fine as well.

```{r echo=FALSE}
pp_check(thiomon_data_model_pooled, type = 'loo_pit_qq', ndraws = 4000)
```

LOO-PIT ECDF is within the blue recommendation.

```{r echo=FALSE}
pp_check(thiomon_data_model_pooled, type = 'pit_ecdf', ndraws = 4000)
```

### 3.1.6 Model parameters

The value of the population intercept 'alpha' is estimated at 0.14. We exclude the likelihood parameters.

```{r echo=FALSE}
as_draws_df(thiomon_data_model_pooled)  |> subset_draws(variable=c('lprior','lp__'), exclude=TRUE) |> summarise_draws() |> tt()
```

## 3.2. Separate model

Try with a separate model. Let's try with the sample one below using the recommended parameters from the above section:

```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
ref_thiomon_formulae_separate <- bf(remission ~ 0 + hgbrat + lymph_percent, family = "bernoulli")

thiomon_priors_default_priors_separate <- get_prior(ref_thiomon_formulae_separate, data = train_data)

(thiomon_set_priors_informative_separate <- c(
  prior(
    normal(-2,5),
    class = "b"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "lymph_percent"
  )
))

thiomon_data_model_separate <- brm(
    formula = ref_thiomon_formulae_separate,
    prior = thiomon_set_priors_informative_separate,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,  
    control = list(
      adapt_delta = 0.9,
      max_treedepth = 20
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
})})
```

### 3.2.1 Checking if the covariates are normally distributed

```{r echo=FALSE}
train_data |>
  ggplot(aes(x = hgb)) + xlim(c(0,50)) +
  geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "MCMC Histogram of Haemoglobin", x = "Parameter Value", y = "Frequency")
```

```{r echo=FALSE}
train_data |>
  ggplot(aes(x = lymph_percent)) + xlim(c(0,75)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "MCMC Histogram of Haemocrit", x = "Parameter Value", y = "Frequency")
```

Both the parameters demonstrate a normal distribution around their respective means. We can approximate with normal distribution for all the parameters.

We don't use flat priors as they are uninformative and we have lots of data to give us good reference values. Also with flat priors, posterior prior checking tools due to input containing infinite or NA values as there is a constant tail. We notice also that the entire population parameters are assigned the same priors. We want to change it. Also, notice that we don't have a population level Intercept.

### 3.2.2 Checking prior sensitivity

With our chosen priors (we used the default priors first and then used the posterior of the slopes to judge what we need), we expect to see no prior sensitivity and we are right. Power-scaling with cumulative Jensen-Shannon distance diagnostic indicates no prior data conflict.

```{r echo=FALSE}
thiomon_data_model_separate |>
  powerscale_plot_dens(help_text=FALSE)
```

```{r echo=FALSE}
thiomon_data_model_separate |>
  powerscale_sensitivity() |> tt()
```

### 3.2.3 R-hat and Effective Sample Size (ESS)

First checking the model for any discrepancies in R-hat and ESS before proceeding any further.

```{r echo=FALSE}
thiomon_data_model_separate
```

R-hat and effective sample sizes look good. At convergence also R-hat value is 1 means the chains have converged well.

```{r echo=FALSE}
loo(thiomon_data_model_separate)
```

### 3.2.4 Issue with Pareto K-hat and Solution

```{r echo=FALSE}
loo(thiomon_data_model_separate)
```

In case there are complaints about Pareto k-estimates, we use the following command and refit the model by removing the observations causing the conflict. Here the model was refit 1 times as there was 1 value with a value between 0.7 and 1.

```{r echo=FALSE}
thiomon_data_model_separate <- add_criterion(thiomon_data_model_separate, criterion='loo', reloo=TRUE)
```

The Pareto k-estimates indicate there are no observations that have k \> 0.7. Posterior predictive checks seem fine as well. Bars plots seem to fit well.

### 3.2.5 Posterior Predictive Checking

```{r echo=FALSE}
pp_check(thiomon_data_model_separate, type = 'bars', ndraws = 200)
```

LOO-PIT check seems fine as well.

```{r echo=FALSE}
pp_check(thiomon_data_model_separate, type = 'loo_pit_qq', ndraws = 4000)
```

LOO-PIT ECDF is within the recommendation.

```{r echo=FALSE}
pp_check(thiomon_data_model_separate, type = 'pit_ecdf', ndraws = 4000)
```

### 3.2.6 Model parameters

The slopes for the covariates can be listed using the following.

```{r echo=FALSE}
as_draws_df(thiomon_data_model_separate)  |> subset_draws(variable=c('lprior','lp__'), exclude=TRUE) |> summarise_draws() |> tt()
```

## 3.3. Hierarchical model

Let's try a hierarchical model. In 'hier_1' there is no global intercept but in 'hier_2' there is.

### 3.3.1. Hierarchical model without global concept

```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
ref_thiomon_formulae_hier_1 <- bf(remission ~ 0 + lymph_percent + (1 | hgbrat), family = "bernoulli")

thiomon_priors_default_priors_hier_1 <- get_prior(ref_thiomon_formulae_hier_1, data = train_data)

thiomon_set_priors_informative_hier_1 <- c(
  
  prior(
    normal(2.5,4),
    class = "sd"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "lymph_percent"
  )
)

thiomon_data_model_hier_1 <- brm(
    formula = ref_thiomon_formulae_hier_1,
    prior = thiomon_set_priors_informative_hier_1,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,            
    control = list(
      adapt_delta = 0.95,    
      max_treedepth = 20 
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
})})
```

### 3.3.2. Hierarchical model with global concept

Note we have also set an informative prior on the Intercept as flat priors cause problems in loo checking due to Pareto K-hat values.

```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
ref_thiomon_formulae_hier_2 <- bf(remission ~ 1 + lymph_percent + (1 | hgbrat), family = "bernoulli")

thiomon_priors_default_priors_hier_2 <- get_prior(ref_thiomon_formulae_hier_2, data = train_data)

thiomon_set_priors_informative_hier_2 <- c(
  
  prior(
    normal(-1.5,2),
    class = "Intercept"
  ),
  
  prior(
    normal(1.5,3),
    class = "sd"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "lymph_percent"
  )
)

thiomon_data_model_hier_2 <- brm(
    formula = ref_thiomon_formulae_hier_2,
    prior = thiomon_set_priors_informative_hier_2,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.95,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
})})
```

### 3.3.3 R-hat and Effective Sample Size (ESS)

R-hat and ESS values look good. Convergence at the end of the run indicates R-hat is 1.0 which means samples are reliable and can be used for inference. Also, posterior prior sensitivity is well adjusted.

```{r echo=FALSE}
thiomon_data_model_hier_1 |>
  powerscale_sensitivity()
```

The prior sensitivity caused some issues for us. There were a few divergent transactions reported (less than 10) but R-hat values were \<= 1.01 for all the parameters. Also, the model reported that R-hat values converged at the end of the simulation. When there are those many divergent transactions, we could have fixed them by increasing the 'adapt_delta' but we chose to check the priors and see if they could be tuned. When tuned too tight the diagnostic would reveal a prior-data conflict and when too wide it would complain about likelihood and prior conflict. We had to iterate to get the best balance of reducing the divergent transactions and the prior conflict. The values -1.5 for Intercept and 1.5 for sd are a testament to this iterative checking.

The following commands are generic for both the hierarchical model choices. We continue with hier_2 as both the models have almost the same EPLD value (negligible difference as expected).

```{r echo=FALSE}
thiomon_data_model_hier_2 |>
  powerscale_sensitivity()
```

### 3.3.4 Issue with Pareto K-hat and Solution

```{r echo=FALSE}
summary(thiomon_data_model_hier_1)
loo(thiomon_data_model_hier_1)
```

```{r echo=FALSE}
summary(thiomon_data_model_hier_2)
loo(thiomon_data_model_hier_2)
```

Since there were no values with problematic K-hat the following is not required.

```{r }
thiomon_data_model_hier_1 <- add_criterion(thiomon_data_model_hier_1, criterion='loo', reloo=TRUE)
```

```{r}
thiomon_data_model_hier_2 <- add_criterion(thiomon_data_model_hier_2, criterion='loo', reloo=TRUE)
```

### 3.3.5 LOO-PIT check

```{r echo=FALSE}
pp_check(thiomon_data_model_hier_1, type = 'loo_pit_qq', ndraws = 4000)
```

```{r echo=FALSE}
pp_check(thiomon_data_model_hier_2, type = 'loo_pit_qq', ndraws = 4000)
```

Try to see if a better hierarchical model fits. The 'sd' prior required a bit of tweaking. Keeping it wider results in 1 divergent transaction which we try to avoid by making it narrow but not quite narrow that we get a prior-data conflict. With this balancing, there are no problems we spot.

There is no issue any more of prior sensitivity.

## 3.4 Pooled, Separate, and Hierarchical Model Comparison

### 3.4.1 LOO comparing all the 4 models

```{r echo=FALSE}
loo_compare(loo(thiomon_data_model_pooled), loo(thiomon_data_model_separate), loo(thiomon_data_model_hier_1), loo(thiomon_data_model_hier_2)) |>
  as.data.frame() |>
  rownames_to_column("model") |>
  dplyr::select(model, elpd_diff, se_diff) |>
  tt()
```

The best-performing model is the hierarchical, closely followed by the separate with hardly much difference in EPLD (less than 0.5), and finally the pooled (with an EPLD score 18 points below both of the hierarchical and separate models).

### 3.4.2 Model posterior distributions

```{r echo=FALSE}
ph1 <- thiomon_data_model_hier_1 |>
  spread_rvars(b_lymph_percent, r_hgbrat[hgbrat,]) |>
  mutate(mean_value = b_lymph_percent + r_hgbrat) |>
  ggplot(aes(xdist= inv_logit_scaled(mean_value), y=hgbrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb Ratio', title='Hierarchical')


ph2 <- thiomon_data_model_hier_2 |>
  spread_rvars(b_Intercept, b_lymph_percent, r_hgbrat[hgbrat,]) |>
  mutate(mean_value = b_Intercept + b_lymph_percent + r_hgbrat) |>
  ggplot(aes(xdist= inv_logit_scaled(mean_value), y=hgbrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb Ratio', title='Hierarchical')

ps_part <- thiomon_data_model_separate |> 
  spread_rvars(b_lymph_percent)

ps_part_fin <- ps_part[rep(1:nrow(ps_part), each = 5), ]

ps <- thiomon_data_model_separate |>
  as_draws_df() |>
  subset_draws(variable = "b_hgbrat", regex = TRUE) |>
  set_variables(paste0('b_hgbrat[', 1:5, ']')) |>
  as_draws_rvars() |>
  spread_rvars(b_hgbrat[hgbrat]) |> cbind(ps_part_fin) |>
  mutate(mean_value = b_hgbrat + b_lymph_percent) |>
  ggplot(aes(xdist=inv_logit_scaled(mean_value), y=hgbrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb Ratio', title='Separate')

pp <- thiomon_data_model_pooled |>
  spread_rvars(b_Intercept) |>
  mutate(mean_value = b_Intercept) |>
  ggplot(aes(xdist=inv_logit_scaled(mean_value), y=0)) +
  stat_halfeye() +
  scale_y_continuous(breaks=NULL) +
  labs(x='Remission factor', y='Across all', title='Pooled model')

(ph1 / ps / pp) * xlim(c(0.0,1.0))

```

The reasons are mostly clear - the separate and hierarchical models perform almost the same but the pooled model does not factor in any covariates and hence only relies on the remission bits as information and hence it has poor predictive power. The close performance of the separate and hierarchical models is because the patients segregated by the 'hgbrat' do share some information mutually in the hierarchical model but this is not much which means that the predicted remissions are not widely varying.

### 3.4.3 Bayes and LOO R2 checks

```{r warning=FALSE, echo=FALSE}
#Pooled model
loo_R2_pooled <- loo_R2(thiomon_data_model_pooled)
Bayes_R2_pooled <- bayes_R2(thiomon_data_model_pooled)

#Separate model
loo_R2_separate <- loo_R2(thiomon_data_model_separate)
Bayes_R2_separate <- bayes_R2(thiomon_data_model_separate)

#Hierarchical model 1
loo_R2_hier_1 <- loo_R2(thiomon_data_model_hier_1)
Bayes_R2_hier_1 <- bayes_R2(thiomon_data_model_hier_1)

#Hierarchical model 2
loo_R2_hier_2 <-loo_R2(thiomon_data_model_hier_2)
Bayes_R2_hier_2 <-bayes_R2(thiomon_data_model_hier_2)
```

The pooled model has negligible values as they don't have any covariates factored in. The separate and hierarchical models do have 14% and 15% explanatory power of the variance of the data. Again this shows the separate and hierarchical models do not differ much here.

The LOO-R2 and Bayes-R2 values both match for all the models indicating that there is no likely over-fitting or under-fitting of the data.

## 3.5. Non-Bayesian model

Besides the implementation of the Bayesian model, we would like to introduce the machine learning aspect to the model as well for comparison between the Bayesian approach and the simple machine learning approach, while finding and evaluating the correlation between model parameters for saving computational resources and suggesting the observation that we can try in Bayesian model as well. General Linear model (GLM) will be described with the following parameters based on the abbreviation of dataset description /cite something here. In general, the model will use all of the parameters of the dataset, without any modification, for comparison only.

```{r}
model_glm <- glm(remission ~ 0 + days_of_life + wbc + hgb + hct + plt + rbc + mcv + mch + mchc + rdw + mpv + neut_percent + lymph_percent + mono_percent + eos_percent + baso_percent + sod + pot + chlor + co2 + un + creat + gluc + cal + prot + alb + ast + alt + alk + tbil, data = thiomon)
```

# 4. Evaluation results

After building the model, due to the objective of the model being classification, we can use the classification evaluation metrics aspect for the evaluation of the model result. Two suggestions that we have implemented: using a confusion matrix with evaluation metrics mathematical calculation, and using ROC curves with AUC values for different model comparisons, which can be elaborated on in session 4.1. and 4.2.

## 4.1. Confusion matrix and Evaluation metrics results

Confusion matrix is a common method for binary classification tasks, where the prediction will be built in the augmented matrix, describing the actual status of the model prediction. Here are the detailed results for each model:

### 4.1.1. Pooled Model

```{r echo=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_pooled <- posterior_predict(thiomon_data_model_pooled, newdata = train_data, draws = 10000, allow_new_levels = TRUE)
train_mean_probs_pooled <- colMeans(train_predicted_probs_pooled)

# Find optimal threshold using ROC curve on training data
train_roc_curve_pooled <- roc(train_data$remission, train_mean_probs_pooled)
optimal_threshold_pooled <- coords(train_roc_curve_pooled, "best", ret = "threshold")

# Predict on test data
predicted_probs_pooled <- posterior_predict(thiomon_data_model_pooled, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_pooled <- colMeans(predicted_probs_pooled)

# ROC Curve
roc_curve_pooled <- roc(test_data$remission, mean_probs_pooled)
# plot(roc_curve_pooled, main = "ROC Curve for Separate Model")

# Confusion Matrix
predicted_classes_pooled <- ifelse(mean_probs_pooled >= optimal_threshold_pooled$threshold, 1, 0)
conf_matrix_pooled <- confusionMatrix(as.factor(predicted_classes_pooled), as.factor(test_data$remission))
print(conf_matrix_pooled)

```

The accuracy of the pooled model is close to random guessing, as indicated by the balanced accuracy of approximately 50%

### 4.1.2. Separate model

```{r echo=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_separate <- posterior_predict(thiomon_data_model_separate, newdata = train_data, draws = 10000, allow_new_levels = TRUE)
train_mean_probs_separate <- colMeans(train_predicted_probs_separate)

# Find optimal threshold using ROC curve on training data
train_roc_curve_separate <- roc(train_data$remission, train_mean_probs_separate)
optimal_threshold_separate <- coords(train_roc_curve_separate, "best", ret = "threshold")

# Predict on test data
predicted_probs_separate <- posterior_predict(thiomon_data_model_separate, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_separate <- colMeans(predicted_probs_separate)

# ROC Curve
roc_curve_separate <- roc(test_data$remission, mean_probs_separate)
# plot(roc_curve_separate, main = "ROC Curve for Separate Model")

# Confusion Matrix
predicted_classes_separate <- ifelse(mean_probs_separate >= optimal_threshold_separate$threshold, 1, 0)
conf_matrix_separate <- confusionMatrix(as.factor(predicted_classes_separate), as.factor(test_data$remission))
print(conf_matrix_separate)
```

The separate model performs better than the pooled model. The model has a higher sensitivity, making it better at identifying true positives.

### 4.1.3. Hierarchical model with global concept

```{r echo=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_hierarchical_1 <- posterior_predict(thiomon_data_model_hier_1, newdata = train_data, draws = 10000, allow_new_levels = TRUE)
train_mean_probs_hierarchical_1 <- colMeans(train_predicted_probs_hierarchical_1)

# Find optimal threshold using ROC curve on training data
train_roc_curve_hierarchical_1 <- roc(train_data$remission, train_mean_probs_hierarchical_1)
optimal_threshold_hierarchical_1 <- coords(train_roc_curve_hierarchical_1, "best", ret = "threshold")

# Predict on test data
predicted_probs_hierarchical_1 <- posterior_predict(thiomon_data_model_hier_1, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_hierarchical_1 <- colMeans(predicted_probs_hierarchical_1)

# ROC Curve
roc_curve_hierarchical_1 <- roc(test_data$remission, mean_probs_hierarchical_1)
# plot(roc_curve_hierarchical_1, main = "ROC Curve for First Model")

# Confusion Matrix
predicted_classes_hierarchical_1 <- ifelse(mean_probs_hierarchical_1 >= optimal_threshold_hierarchical_1$threshold, 1, 0)
conf_matrix_hierarchical_1 <- confusionMatrix(as.factor(predicted_classes_hierarchical_1), as.factor(test_data$remission))
print(conf_matrix_hierarchical_1)
```

This hierarchical model has an accuracy of nearly 60% (which may be changed when knitting the docume). The sensitivity is higher than the specificity, indicating that the model is better at identifying true positives.

### 4.1.4. Hierarchical model without global concept

```{r echo=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_hierarchical_2 <- posterior_predict(thiomon_data_model_hier_2, newdata = train_data, draws = 10000, allow_new_levels = TRUE)
train_mean_probs_hierarchical_2 <- colMeans(train_predicted_probs_hierarchical_2)

# Find optimal threshold using ROC curve on training data
train_roc_curve_hierarchical_2 <- roc(train_data$remission, train_mean_probs_hierarchical_2)
optimal_threshold_hierarchical_2 <- coords(train_roc_curve_hierarchical_2, "best", ret = "threshold")

# Predict on test data
predicted_probs_hierarchical_2 <- posterior_predict(thiomon_data_model_hier_2, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_hierarchical_2 <- colMeans(predicted_probs_hierarchical_2)

# ROC Curve
roc_curve_hierarchical_2 <- roc(test_data$remission, mean_probs_hierarchical_2)
# plot(roc_curve_hierarchical_2, main = "ROC Curve for First Model")

# Confusion Matrix
predicted_classes_hierarchical_2 <- ifelse(mean_probs_hierarchical_2 >= optimal_threshold_hierarchical_2$threshold, 1, 0)
conf_matrix_hierarchical_2 <- confusionMatrix(as.factor(predicted_classes_hierarchical_2), as.factor(test_data$remission))
print(conf_matrix_hierarchical_2)
```

Hierarchical model 2 performs approximately with hierarchical model 1. The sensitivity is higher, indicating better identification of true positives.

### 4.1.5. GLM Model

```{r echo=FALSE}
# Predict probabilities on the training data
pred_probs_glm <- predict(model_glm, train_data, type = "response")

# Generate the ROC curve
train_roc_curve_glm <- roc(train_data$remission, pred_probs_glm)

# Find the optimal threshold
optimal_threshold_glm <- coords(train_roc_curve_glm, "best", ret = "threshold")

# Apply the threshold to the test_data
pred_probs_glm_test <- predict(model_glm, test_data, type = "response") 

# Convert probabilities to class labels using the optimal threshold
pred_classes_glm_test <- ifelse(pred_probs_glm_test > optimal_threshold_glm$threshold, 1, 0)

# Confusion matrix using the optimal_threshold_glm
conf_matrix_glm <- confusionMatrix(as.factor(pred_classes_glm_test), as.factor(test_data$remission))
print(conf_matrix_glm)

# Plot the ROC curve
roc_curve_glm <- roc(test_data$remission, pred_probs_glm_test)
# plot(roc_curve_glm, main = "ROC Curve for GLM Model")
# Plot the ROC curve
plot(roc_curve_glm, main = "ROC Curve for GLM Model")
```

The GLM model outperforms all other models with an accuracy of over 65%. The model has a good balance between sensitivity and specificity, making it effective at identifying both true positives and true negatives.

## 4.2. ROC curves and AUC values

```{r echo=FALSE}
# Function to interpolate and smooth ROC curve with jitter
interpolate_smooth_roc_jitter <- function(roc_curve, n = 100, jitter_amount = 1e-5) {
  roc_smooth <- smooth(roc_curve)
  fpr <- seq(0, 1, length.out = n)
  tpr <- approx(roc_smooth$specificities + runif(length(roc_smooth$specificities), -jitter_amount, jitter_amount), 
                roc_smooth$sensitivities, xout = fpr)$y
  data.frame(fpr = fpr, tpr = tpr)
}

# Calculate and interpolate ROC curves with jitter
roc_pooled_train <- interpolate_smooth_roc_jitter(train_roc_curve_pooled)
roc_pooled_test <- interpolate_smooth_roc_jitter(roc_curve_pooled)
roc_separate_train <- interpolate_smooth_roc_jitter(train_roc_curve_separate)
roc_separate_test <- interpolate_smooth_roc_jitter(roc_curve_separate)
roc_hierarchical_1_train <- interpolate_smooth_roc_jitter(train_roc_curve_hierarchical_1)
roc_hierarchical_1_test <- interpolate_smooth_roc_jitter(roc_curve_hierarchical_1)
roc_hierarchical_2_train <- interpolate_smooth_roc_jitter(train_roc_curve_hierarchical_2)
roc_hierarchical_2_test <- interpolate_smooth_roc_jitter(roc_curve_hierarchical_2)

# glm, svm and nn model too
roc_glm_train <- interpolate_smooth_roc_jitter(train_roc_curve_glm)
roc_glm_test <- interpolate_smooth_roc_jitter(roc_curve_glm)

# Calculate AUC values
auc_pooled_train <- auc(train_roc_curve_pooled)
auc_pooled_test <- auc(roc_curve_pooled)
auc_separate_train <- auc(train_roc_curve_separate)
auc_separate_test <- auc(roc_curve_separate)
auc_hierarchical_1_train <- auc(train_roc_curve_hierarchical_1)
auc_hierarchical_1_test <- auc(roc_curve_hierarchical_1)
auc_hierarchical_2_train <- auc(train_roc_curve_hierarchical_2)
auc_hierarchical_2_test <- auc(roc_curve_hierarchical_2)
auc_glm_train <- auc(train_roc_curve_glm)
auc_glm_test <- auc(roc_curve_glm)

# Combine ROC curves into one data frame

train_roc_data <- rbind(
  data.frame(roc_pooled_train, model = paste("Pool Train (AUC =", round(auc_pooled_train, 2), ")")),
  data.frame(roc_separate_train, model = paste("Separate Train (AUC =", round(auc_separate_train, 2), ")")),
  data.frame(roc_hierarchical_1_train, model = paste("Hierarchical with Global Intercept Train (AUC =", round(auc_hierarchical_1_train, 2), ")")),
  data.frame(roc_hierarchical_2_train, model = paste("Hierarchical without Global Intercept Train (AUC =", round(auc_hierarchical_2_train, 2), ")")),
  data.frame(roc_glm_train, model = paste("GLM Train (AUC =", round(auc_glm_train, 2), ")"))
  )
test_roc_data <- rbind(
  data.frame(roc_pooled_test, model = paste("Pool Test (AUC =", round(auc_pooled_test, 2), ")")),
  data.frame(roc_separate_test, model = paste("Separate Test (AUC =", round(auc_separate_test, 2), ")")),
  data.frame(roc_hierarchical_1_test, model = paste("Hierarchical with Global Intercept Test (AUC =", round(auc_hierarchical_1_test, 2), ")")),
  data.frame(roc_hierarchical_2_test, model = paste("Hierarchical without Global Intercept Test (AUC =", round(auc_hierarchical_2_test, 2), ")")),
  data.frame(roc_glm_test, model = paste("GLM Test (AUC =", round(auc_glm_test, 2), ")"))
  )
# Plot all ROC curves in one plot
ggplot(train_roc_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line() +
  labs(title = "ROC Curves for Different Models", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()
ggplot(test_roc_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line() +
  labs(title = "ROC Curves for Different Models", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()

```

According to the evaluation result, we could see that almost all the models have an AUC difference between training and testing data maximum of 7%, which can be an acceptable value. Moreover, the Bayesian approach cannot be better than the GLM approach. One of the proposals for this is because of having less occupied parameters in the model. We can base on that to improve our model as well.

## 4.3. Discussion.

According to the evaluation results, the GLM model outperforms the Bayesian models in terms of accuracy and AUC. Moreover, as expected from the Bayesian analysis, the difference between Separate and Hierarchical here is not clear. The Bayesian models show moderate performance but do not surpass the GLM model.

# 5. Discussion of Issues and Potential Improvements

The main issue observed is the moderate performance of the Bayesian models compared to the GLM model. To improve the accuracy of the Bayesian models, we can consider increasing the number of variables, introducing different methods for approaching the prior, or normalizing the parameters into different categories and medical terms. This can help improve the model's accuracy and predictive power.

## 5.1. Improving the number of training sample

An example of applying 4500 variables into a separate model.

```{r include = FALSE}
#Process the data set with some additional processing
source_data <- thiomon

#Take the ratio of hgb and hct which describes if the patient is anaemic or not
source_data$hbghctrat = round(thiomon$hgb/thiomon$hct,2)

#Group into 5 groups
source_data$hbghctrat <- cut(source_data$hbghctrat, breaks = seq(min(source_data$hbghctrat), max(source_data$hbghctrat), length.out = 6), labels = c(1,2,3,4,5))

#Convert this to a character label
source_data$hbghctrat <- as.character(source_data$hbghctrat)

source_data$hgbratnum <- cut(source_data$hgb, breaks = seq(min(source_data$hgb), max(source_data$hgb), length.out = 6), labels = c(1,2,3,4,5))
source_data$hgbrat <- as.character(source_data$hgbratnum)
# Sample a subset of the 4500 rows to improve computation time
MAX_DATA_SAMPLES <- 4500
#Number of samples for the likelihood
NUM_DATA_SAMPLES <- 4500
source_data <- na.omit(source_data)
train_data_norm <- source_data[sample(MAX_DATA_SAMPLES, NUM_DATA_SAMPLES), ]

train_data <- train_data_norm
train_data <- na.omit(train_data)
test_data <- source_data[4501:nrow(source_data), ]
```

```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
ref_thiomon_formulae_separate <- bf(remission ~ 0 + hgbrat + lymph_percent, family = "bernoulli")

thiomon_priors_default_priors_separate <- get_prior(ref_thiomon_formulae_separate, data = train_data)

(thiomon_set_priors_informative_separate <- c(
  prior(
    normal(-2,5),
    class = "b"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "lymph_percent"
  )
))

thiomon_data_model_separate_improvement <- brm(
    formula = ref_thiomon_formulae_separate,
    prior = thiomon_set_priors_informative_separate,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.9,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
})})
```

```{r include=FALSE}
thiomon_data_model_separate_improvement |>
  powerscale_plot_dens(help_text=FALSE)
thiomon_data_model_separate_improvement |>
  powerscale_sensitivity() |> tt()

thiomon_data_model_separate_improvement

loo(thiomon_data_model_separate_improvement)
thiomon_data_model_separate_improvement <- add_criterion(thiomon_data_model_separate_improvement, criterion='loo', reloo=TRUE)
pp_check(thiomon_data_model_separate_improvement, type = 'bars', ndraws = 200)
pp_check(thiomon_data_model_separate_improvement, type = 'loo_pit_qq', ndraws = 4000)
pp_check(thiomon_data_model_separate_improvement, type = 'pit_ecdf', ndraws = 4000)
as_draws_df(thiomon_data_model_separate_improvement)  |> subset_draws(variable=c('lprior','lp__'), exclude=TRUE) |> summarise_draws() |> tt()
```

All of the Bayesian check methods have been produced internally and did not provide any problems compared to the previous model. Therefore, we could come to an evaluation session

```{r echo=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_separate_improvement <- posterior_predict(thiomon_data_model_separate_improvement, newdata = train_data, draws = 10000)
train_mean_probs_separate_improvement <- colMeans(train_predicted_probs_separate_improvement)

# Find optimal threshold using ROC curve on training data
train_roc_curve_separate_improvement <- roc(train_data$remission, train_mean_probs_separate_improvement)
optimal_threshold_separate_improvement <- coords(train_roc_curve_separate_improvement, "best", ret = "threshold")

# Predict on test data
predicted_probs_separate_improvement <- posterior_predict(thiomon_data_model_separate_improvement, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_separate_improvement <- colMeans(predicted_probs_separate_improvement)

# ROC Curve
roc_curve_separate_improvement <- roc(test_data$remission, mean_probs_separate_improvement)
# plot(roc_curve_separate, main = "ROC Curve for Separate Model")

# Confusion Matrix
predicted_classes_separate_improvement <- ifelse(mean_probs_separate_improvement >= optimal_threshold_separate_improvement$threshold, 1, 0)
conf_matrix_separate_improvement <- confusionMatrix(as.factor(predicted_classes_separate_improvement), as.factor(test_data$remission))
print(conf_matrix_separate_improvement)
```

Based on observation, the actual accuracy of the model here is larger, but not significant. However, the generalization of the model is better due to the decrease in the sensitivity and increase and specificity. This is a good sign for the model as well.

Moreover, a comparison between using less and more data in ROC/AUC values can be presented below.

## 5.2. Increasing the number of parameters in the original model.

The model here can be computed with more parameters, with the purpose of producing a better result. However, using more parameters can also lead to an increase in the model complexity, which may not be appropriate for the task's project. Therefore, an example of changing the separate model formula with adding 4 more parameters hct, wbc, mpv, mchc into the model and we can observe the result.

```{r include = FALSE}
#Process the data set with some additional processing
source_data <- thiomon

#Take the ratio of hgb and hct which describes if the patient is anaemic or not
source_data$hbghctrat = round(thiomon$hgb/thiomon$hct,2)

#Group into 5 groups
source_data$hbghctrat <- cut(source_data$hbghctrat, breaks = seq(min(source_data$hbghctrat), max(source_data$hbghctrat), length.out = 6), labels = c(1,2,3,4,5))

#Convert this to a character label
source_data$hbghctrat <- as.character(source_data$hbghctrat)

source_data$hgbratnum <- cut(source_data$hgb, breaks = seq(min(source_data$hgb), max(source_data$hgb), length.out = 6), labels = c(1,2,3,4,5))
source_data$hgbrat <- as.character(source_data$hgbratnum)
# Sample a subset of the 4500 rows to improve computation time
MAX_DATA_SAMPLES <- 4500
#Number of samples for the likelihood
NUM_DATA_SAMPLES <- 2000
source_data <- na.omit(source_data)
train_data_norm <- source_data[sample(MAX_DATA_SAMPLES, NUM_DATA_SAMPLES), ]

train_data <- train_data_norm
train_data <- na.omit(train_data)
test_data <- source_data[4501:nrow(source_data), ]
```

```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
ref_thiomon_formulae_separate <- bf(remission ~ 0 + hgbrat + lymph_percent + hct + wbc + mpv + mchc, family = "bernoulli")

thiomon_priors_default_priors_separate <- get_prior(ref_thiomon_formulae_separate, data = train_data)

(thiomon_set_priors_informative_separate <- c(
  prior(
    normal(19.5,20),
    class = "b",
    coef = "lymph_percent"
  ),
  
  prior(
    normal(7.5,10),
    class = "b",
    coef = "wbc"
  ),
  
  prior(
    normal(34,5),
    class = "b",
    coef = "mchc"
  ),

  prior(
    normal(37.1,10),
    class = "b",
    coef = "hct"
  ),
  
  prior(
    normal(8.2,5),
    class = "b",
    coef = "mpv"
  )
))

thiomon_data_model_separate_improvement_1 <- brm(
    formula = ref_thiomon_formulae_separate,
    prior = thiomon_set_priors_informative_separate,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.9,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
})})
```

```{r include=FALSE}
thiomon_data_model_separate_improvement_1 |>
  powerscale_plot_dens(help_text=FALSE)
thiomon_data_model_separate_improvement_1 |>
  powerscale_sensitivity() |> tt()

thiomon_data_model_separate_improvement_1

loo(thiomon_data_model_separate_improvement_1)
thiomon_data_model_separate_improvement_1 <- add_criterion(thiomon_data_model_separate_improvement_1, criterion='loo', reloo=TRUE)
pp_check(thiomon_data_model_separate_improvement_1, type = 'bars', ndraws = 200)
pp_check(thiomon_data_model_separate_improvement_1, type = 'loo_pit_qq', ndraws = 4000)
pp_check(thiomon_data_model_separate_improvement_1, type = 'pit_ecdf', ndraws = 4000)
as_draws_df(thiomon_data_model_separate_improvement_1)  |> subset_draws(variable=c('lprior','lp__'), exclude=TRUE) |> summarise_draws() |> tt()
```

```{r echo=FALSE}
# Calculate predicted probabilities on training data
train_predicted_probs_separate_improvement_1 <- posterior_predict(thiomon_data_model_separate_improvement_1, newdata = train_data, draws = 10000)
train_mean_probs_separate_improvement_1 <- colMeans(train_predicted_probs_separate_improvement_1)

# Find optimal threshold using ROC curve on training data
train_roc_curve_separate_improvement_1 <- roc(train_data$remission, train_mean_probs_separate_improvement_1)
optimal_threshold_separate_improvement_1 <- coords(train_roc_curve_separate_improvement_1, "best", ret = "threshold")

# Predict on test data
predicted_probs_separate_improvement_1 <- posterior_predict(thiomon_data_model_separate_improvement_1, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_separate_improvement_1 <- colMeans(predicted_probs_separate_improvement_1)

# ROC Curve
roc_curve_separate_improvement_1 <- roc(test_data$remission, mean_probs_separate_improvement_1)
# plot(roc_curve_separate, main = "ROC Curve for Separate Model")

# Confusion Matrix
predicted_classes_separate_improvement_1 <- ifelse(mean_probs_separate_improvement_1 >= optimal_threshold_separate_improvement_1$threshold, 1, 0)
conf_matrix_separate_improvement_1 <- confusionMatrix(as.factor(predicted_classes_separate_improvement_1), as.factor(test_data$remission))
print(conf_matrix_separate_improvement_1)
```

## 5.3. R2D2 method for the priors and using the full set of covariates for the modeling

Moreover, besides these trials that have been implemented, there can be room for improvement. An example is normalizing the parameters into medical terms for building a model with efficient parameters only. Furthermore, we have not tried to use with spline approach in the model, or with multi-parameters in hierarchical models. They are worth considering when we approach the problems and can be the necessary steps for improvement.

One improvement that we consider is to use the R2D2 method for the priors and use the full set of covariates for the modeling.

```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
formula_r2d2 <- bf(remission ~ 0 + wbc + hgb + hct + plt + rbc + mcv + mch + mchc + rdw + mpv + neut_percent + lymph_percent + mono_percent + eos_percent + baso_percent + sod + pot + chlor + co2 + un + creat + gluc + cal + prot + alb + ast + alt + alk + tbil, family = "bernoulli")

fit_r2d2 <- brm(formula_r2d2, data = train_data,
            normalize = FALSE,
            prior=c(prior(R2D2(mean_R2 = 0.5, prec_R2 = 1, cons_D2 = 1,
                               autoscale = FALSE),class=b)),
            threads = threading(2), cores = 8, backend = "cmdstanr")
})})
```

```{r}
p_fit_r2d2 <- fit_r2d2 |> as_draws_df() |> as_draws_rvars() |> spread_rvars(b_wbc, b_hgb, b_hct, b_plt, b_rbc, b_mcv, b_mch, b_mchc, b_rdw, b_mpv, b_neut_percent, b_lymph_percent, b_mono_percent, b_eos_percent, b_baso_percent, b_sod, b_pot, b_chlor, b_co2, b_un, b_creat, b_gluc, b_cal, b_prot, b_alb, b_ast, b_alt, b_alk, b_tbil) |> mutate(mean_value = b_wbc + b_hgb + b_hct + b_plt + b_rbc + b_mcv + b_mch + b_mchc + b_rdw + b_mpv + b_neut_percent + b_lymph_percent + b_mono_percent + b_eos_percent + b_baso_percent + b_sod + b_pot + b_chlor + b_co2 + b_un + b_creat + b_gluc + b_cal + b_prot + b_alb + b_ast + b_alt + b_alk + b_tbil) |>
  ggplot(aes(xdist=inv_logit_scaled(mean_value), y=0)) +
  stat_halfeye() +
  scale_y_continuous(breaks=NULL) +
  labs(x='Remission factor', y='Across all covariates', title='Model with R2D2')
(p_fit_r2d2) * xlim(c(0.0,1.0))
```

```{r echo=FALSE}
train_predicted_probs_r2d2 <- posterior_predict(fit_r2d2, newdata = train_data, draws = 10000, allow_new_levels = TRUE)
train_mean_probs_r2d2 <- colMeans(train_predicted_probs_r2d2)

# Find optimal threshold using ROC curve on training data
train_roc_curve_r2d2 <- roc(train_data$remission, train_mean_probs_r2d2)
optimal_threshold_r2d2 <- coords(train_roc_curve_r2d2, "best", ret = "threshold")

# Predict on test data
predicted_probs_r2d2 <- posterior_predict(fit_r2d2, newdata = test_data, draws = 1000, allow_new_levels = TRUE)
mean_probs_r2d2 <- colMeans(predicted_probs_r2d2)

# ROC Curve
roc_curve_r2d2 <- roc(test_data$remission, mean_probs_r2d2)
# plot(roc_curve_pooled, main = "ROC Curve for Separate Model")

# Confusion Matrix
predicted_classes_r2d2 <- ifelse(mean_probs_r2d2 >= optimal_threshold_r2d2$threshold, 1, 0)
conf_matrix_r2d2 <- confusionMatrix(as.factor(predicted_classes_r2d2), as.factor(test_data$remission))

print(conf_matrix_r2d2)
```

## 5.4. ROC curves/AUC values comparison

```{r echo=FALSE}
# Calculate and interpolate ROC curves with jitter
roc_separate_train <- interpolate_smooth_roc_jitter(train_roc_curve_separate)
roc_separate_test <- interpolate_smooth_roc_jitter(roc_curve_separate)
roc_separate_improvement_train <- interpolate_smooth_roc_jitter(train_roc_curve_separate_improvement)
roc_separate_improvement_test <- interpolate_smooth_roc_jitter(roc_curve_separate_improvement)
roc_separate_improvement_1_train <- interpolate_smooth_roc_jitter(train_roc_curve_separate_improvement_1)
roc_separate_improvement_1_test <- interpolate_smooth_roc_jitter(roc_curve_separate_improvement_1)
roc_r2d2_improvement_train <- interpolate_smooth_roc_jitter(train_roc_curve_r2d2)
roc_r2d2_improvement_test <- interpolate_smooth_roc_jitter(roc_curve_r2d2)



# Calculate AUC values
auc_separate_train <- auc(train_roc_curve_separate)
auc_separate_test <- auc(roc_curve_separate)
auc_separate_improvement_train <- auc(train_roc_curve_separate_improvement)
auc_separate_improvement_test <- auc(roc_curve_separate_improvement)
auc_separate_improvement_1_train <- auc(train_roc_curve_separate_improvement_1)
auc_separate_improvement_1_test <- auc(roc_curve_separate_improvement_1)
auc_r2d2_improvement_1_train <- auc(train_roc_curve_r2d2)
auc_r2d2_improvement_1_test <- auc(roc_curve_r2d2)

# Combine ROC curves into one data frame

train_roc_data <- rbind(
  data.frame(roc_separate_train, model = paste("Separate Train (AUC =", round(auc_separate_train, 2), ")")),
  data.frame(roc_separate_improvement_train, model = paste("Separate model + more training samples (AUC =", round(auc_separate_improvement_train, 2), ")")),
  data.frame(roc_separate_improvement_1_train, model = paste("Separate model + more variables (AUC =", round(auc_separate_improvement_1_train, 2), ")")),
  data.frame(roc_r2d2_improvement_train, model = paste("R2D2 prior (AUC =", round(auc_separate_improvement_1_train, 2), ")"))
)
test_roc_data <- rbind(
  data.frame(roc_separate_test, model = paste("Separate Test (AUC =", round(auc_separate_test, 2), ")")),
    data.frame(roc_separate_improvement_test, model = paste("Separate model + more training samples (AUC =", round(auc_separate_improvement_test, 2), ")")),
    data.frame(roc_separate_improvement_1_test, model = paste("Separate model + more variables (AUC =", round(auc_separate_improvement_1_test, 2), ")")),
  data.frame(roc_r2d2_improvement_test, model = paste("R2D2 prior (AUC =", round(auc_separate_improvement_1_train, 2), ")"))
)

# Plot all ROC curves in one plot
ggplot(train_roc_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line() +
  labs(title = "Train ROC Curves for Different Models", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()
ggplot(test_roc_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line() +
  labs(title = "Test ROC Curves for Different Models", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal()
```

As you can observe, more training samples here do not print any differences, while more variables can lead to better model accuracy, having better ROC curves and AUC values. This can be a potential option for us to analyze and improve in the future.

# 6. Conclusion

After evaluating various models on the Thiomon dataset, we can conclude that the effects of the variables regarding remission are considerable. The Bayesian approach for this remission classification task is appropriate. However, the Bayesian approach is computationally complex, where increasing the number of parameters will lead to more time consumption during the fitting process, and the model accuracy will be better proportional to the time training. Therefore, we can expect more accuracy improvement when we build a larger model or use different Bayesian methods, for example, choosing different prior, or applying a hybrid approach in an actual research project.

Compared to these models, the hierarchical models, particularly the one with a global coefficient, provided a good balance between capturing global and group-level variations, leading to the best overall performance. The GLM models also showed promising results with fewer computational resources, which can also be a threshold for us to recompute the Bayesian model in a research project. Moreover, GLM also describes to us the potential of using more parameters in the Bayesian approach can lead to better results, which have also been experimented with in the Potential Improvements section.

# 7. Self-Reflection

Regarding the project, we have learned how to collaborate and help each other in the group. The project has been done with multiple trials and errors, with different parameter fitting. And the final result is the balance trade-off of accuracy and time training. In some cases, some evaluation metrics need knowledge, leading to our discussion and revision of course content material and providing the opportunity for us to improve ourselves throughout the professor's project report. Even though the Bayesian classification result cannot be used in real research work and does not have a good accuracy compared to the other research publications, the application of Bayesian and how to evaluate the model can help us build our fundamental knowledge and inspire our future research path. Moreover, the process of doing the project together can also enhance our skills in teamwork and communication. We appreciate the opportunity from the course orientation to the final project.

# References
