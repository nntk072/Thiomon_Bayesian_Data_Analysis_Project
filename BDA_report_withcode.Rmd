---
title: "Thiomon Data Analysis Report"
author: "Anonymous"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 5
bibliography: references.bib 
csl: vancouver.csl
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}
setwd("/notebooks/bda2024/Bayes_project/Bayes_project")
# setwd("C:/Users/nguyenl37/Downloads/Bayes_project/")
# setwd("C:/Users/nguye/OneDrive - Aalto University/University/First year/Project/Bayes_project")
SEED = 42
if(!require(nnet)) {
  install.packages("nnet")
  library(nnet)
}
if (!require(rstan)) {
    install.packages("rstan")
    library(rstan)
}

if (!require(loo)) {
    install.packages("loo")
    library(loo)
}

if (!require(gridExtra)) {
    install.packages("gridExtra")
    library(gridExtra)
}

if (!require(grid)) {
    install.packages("grid")
    library(grid)
}
if (!require(rmarkdown)) {
    install.packages("rmarkdown")
    library(rmarkdown)
}

if (!require(tidybayes)) {
    install.packages("tidybayes")
    library(tidybayes)
}

if (!require(brms)) {
    install.packages("brms")
    library(brms)
}

if (!require(metadat)) {
  install.packages("metadat")
  library(metadat)
}

if(!require(cmdstanr)){
    install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
    library(cmdstanr)
}

cmdstan_installed <- function(){
  res <- try(out <- cmdstanr::cmdstan_path(), silent = TRUE)
  !inherits(res, "try-error")
}

if(!cmdstan_installed()){
    install_cmdstan()
}

if(!require(ggplot2)){
    install.packages("ggplot2")
    library(ggplot2)
}

ggplot2::theme_set(theme_minimal(base_size = 14))
if(!require(bayesplot)){
    install.packages("bayesplot")
    library(bayesplot)
}

if(!require(posterior)){
    install.packages("posterior")
    library(posterior)
}

if (!require(priorsense)) {
  install.packages("priorsense")
  library(priorsense)
}

if (!require(tibble)) {
  install.packages("tibble")
  library(tibble)
}

if (!require(RColorBrewer)) {
  install.packages("RColorBrewer")
  library(RColorBrewer)
}

if (!require(dplyr)) {
  install.packages("dplyr")
  library(dplyr)
}

if (!require(pROC)) {
  install.packages("pROC")
  library(pROC)
}

if (!require(caret)) {
  install.packages("caret")
  library(caret)
}

if (!require(tinytable)) {
  install.packages("tinytable")
  library(tinytable)
}

options(tinytable_format_num_fmt = "significant_cell", tinytable_format_digits = 2, tinytable_tt_digits=2)
```

```{r include=FALSE}
suppressPackageStartupMessages(library(nnet))
suppressPackageStartupMessages(library(loo))
suppressPackageStartupMessages(library(rstan))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(bayesplot))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(grid))
suppressPackageStartupMessages(library(rmarkdown))
suppressPackageStartupMessages(library(brms))
suppressPackageStartupMessages(library(cmdstanr))
suppressPackageStartupMessages(library(priorsense))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(RColorBrewer))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(posterior))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(pROC))
```

\newpage

(This is an example of using reference) This study explores the use of machine learning algorithms for predicting clinical outcomes with thiopurines [@10.1093/ecco-jcc/jjx014]. you can change the sections/subsections/name according to your idea

# 1. Introduction

Thiopurines are widely used immunomodulators for the treatment of ulcerative colitis [UC] and Crohn's disease [CD] in inflammatory bowel disease [IBD] patients. Thiopurines can induce immune suppression, clinical and biological remission, and are steroid-sparing agents.

# 1.1. Motivation

The study uses a data set based on de-identified Laboratory Data on IBD Patients using Thiourines for at least 4 weeks and their eventual Remission/Active Status after at least 12 Weeks of therapy.

# 1.2. Problem Statement

We plan to apply Bayesian data analysis technique to predict the Objective Remission (OR) in optimizing thiopurine therapy for inflammatory bowel disease by applying pooled, separate and hierarchical models. Furthermore, we make a comparison between the 3 models and draw some observations.

# 1.3. Main Modeling Summary

# 2. Data: Thiomon Dataset

A dataset containing laboratory data and outcomes of IBD patients on Thiopurine therapy at the University of Michigan. Data on laboratory values for a complete blood count and chemistry panel at least 4 weeks after start of thiopurine therapy in IBD patients. The University of Michigan Hospital is in Ann Arbor, USA. These data have been anonymized, and time-shifted. Age is reported in days of life.

## 2.1. Data description

Based on the blood parameters, we can categorize them into broad categories like Hematology, Chemistry, Electrolytes, Liver Function, and Inflammation/Other Markers.

**Hematology Parameters (Related to blood cells, hemoglobin, and related components)**

Platelet Count (plt)

Mean Platelet Volume (mpv)

White Blood Cell Count (wbc)

Hemoglobin (hgb)

Hematocrit (hct)

Red Blood Cell Count (rbc)

Mean Corpuscular Volume (MCV) (mcv)

Mean Corpuscular Hemoglobin (MCH) (mch)

Mean Corpuscular Hemoglobin Concentration (MCHC) (mchc)

Red Cell Distribution Width (RDW) (rdw)

Neutrophil Percent (neut_percent)

Lymphocyte Percent (lymph_percent)

Monocyte Percent (mono_percent)

Eosinophil Percent (eos_percent)

Basophil Percent (baso_percent)

**Chemistry Parameters (General body chemistry markers)**

Blood Urea Nitrogen (BUN) (un)

Sodium (sod)

Potassium (pot)

Chloride (chlor)

Bicarbonate (CO2) (co2)

Creatinine (creat)

Glucose (gluc)

Calcium (cal)

Protein (prot)

Albumin (alb)

**Liver Function Parameters (Enzyme and bilirubin levels)**

Aspartate Transaminase (AST) (ast)

Alanine Transaminase (ALT) (alt)

Alkaline Phosphatase (ALK) (alk)

Total Bilirubin (Tbil) (tbil

**Inflammation and Other Markers**

Active Inflammation Despite Thiopurines for \> 12 Weeks (active)

Remission of Inflammation After Thiopurines for \> 12 Weeks (remission)

## 2.2. Data preprocessing

Load the data comprising of approximately 5K rows of blood chemistry with remission and active status (each is 1 bit and complementary to each other).

```{r}
#Loading the data
load("thiomon.rda")
thiomon <- na.omit(thiomon)
```

```{r}
#Process the data set with some additional processing
source_data <- thiomon

#Take the ratio of hgb and hct which describes if the patient is anaemic or not
source_data$hbghctrat = round(thiomon$hgb/thiomon$hct,2)

#Group into 5 groups
source_data$hbghctrat <- cut(source_data$hbghctrat, breaks = seq(min(source_data$hbghctrat), max(source_data$hbghctrat), length.out = 6), labels = c("1", "2", "3", "4", "5"))

#Convert this to a character label
source_data$hbghctrat <- as.character(source_data$hbghctrat)

#Remove invalid entries
source_data <- na.omit(source_data)

# Sample a subset of the 4500 rows to improve computation time
MAX_DATA_SAMPLES <- 4000

#Number of samples for the likelihood
NUM_DATA_SAMPLES <- 1000
train_data <- source_data[sample(MAX_DATA_SAMPLES, NUM_DATA_SAMPLES), ]

#Number of samples for the testing
test_data <- source_data[4001:5000, ]
```

We choose 1K samples for fitting the model and 1K samples for checking the model performance.

# 3. Priors

## 3.1. Variables characteristics

Showing that the data follow normal distribution \## 3.2. Variables prior description Choosing by mean/std

# 4. Models

## 4.1. Pooled model

Make a basic pooled model.

```{r}
ref_thiomon_formulae_pooled <- bf(remission ~ 1, family = "bernoulli")

thiomon_priors_default_priors_pooled <- get_prior(ref_thiomon_formulae_pooled, data = train_data)

thiomon_set_priors_informative_pooled <- c(
  prior(
    normal(0,1),
    class = "Intercept"
  ) 
)

thiomon_data_model_pooled <- brm(
    formula = ref_thiomon_formulae_pooled,
    prior = thiomon_set_priors_informative_pooled,
    data = train_data,
    save_pars = save_pars(all = TRUE),
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.8,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

There are no divergent transactions found. Let's check for some model related diagnostics.

### 4.1.1 R-hat and Effective Sample Size (ESS)

First checking the model for any discrepancies in R-hat and ESS.

```{r}
thiomon_data_model_pooled
```

R-hat and effective sample sizes look good. At convergence also R-hat value is 1 means the chains have converged well.

### 4.1.2 Pareto k-hat

```{r}
loo(thiomon_data_model_pooled)
```

There are no Pareto k-hat estimated greater than 0.7 which is good.

### 4.1.3 Choosing the prior

Continuing with checks on prior, justification for the chosen prior is that the default prior 'student_t(3, 0, 2.5)' is almost equivalent to a 'normal(0, 1.5)' but we find that the underlying 'theta' of the distribution is '0.48' which allows us to make a narrower choice.

```{r}
data.frame(theta = plogis(rnorm(n=20000, mean=0, sd=1.0))) |>
  mcmc_hist() +
  xlim(c(0,1)) +
  labs(title='normal(0, 1.0) for Intercept') 
```

It seems it is a correct choice as there is no conflict revealed from the prior diagnostics.

### 4.1.4 Posterior prior sensitivity

```{r}
thiomon_data_model_pooled |>
  powerscale_plot_dens(help_text=FALSE)
```

The values themselves are also very small.

```{r}
thiomon_data_model_pooled |>
  powerscale_sensitivity() |> tt()
```

Posterior predictive checks seem fine as well. Density overlay seems to fit well.

```{r}
pp_check(thiomon_data_model_pooled, type = 'dens_overlay', ndraws = 200)
```

### 4.1.5 Posterior predictive checking

LOO-PIT check seems fine as well.

```{r}
pp_check(thiomon_data_model_pooled, type = 'loo_pit_qq', ndraws = 4000)
```

LOO-PIT ECDF is within the blue recommendation.

```{r}
pp_check(thiomon_data_model_pooled, type = 'pit_ecdf', ndraws = 4000)
```

### 4.1.6 Model parameters

Value of the population intercept 'alpha' is estimated at 0.14. We exclude the likelihood parameters.

```{r}
as_draws_df(thiomon_data_model_pooled)  |> subset_draws(variable=c('lprior','lp__'), exclude=TRUE) |> summarise_draws() |> tt()
```

## 4.2. Separate model

Try with the separate model. Let's try with the sample one below:

```{r}
ref_thiomon_formulae_separate <- bf(remission ~ 0 + hgb + hct + hbghctrat, family = "bernoulli")

thiomon_priors_default_priors_separate <- get_prior(ref_thiomon_formulae_separate, data = train_data)

(thiomon_set_priors_informative_separate <- c(
  
  prior(
    normal(3,1),
    class = "b"
  ),
  
  prior(
    normal(35,5),
    class = "b",
    coef = "hct"
  ),
  
  prior(
    normal(13,3),
    class = "b",
    coef = "hgb"
  )
))

thiomon_data_model_separate <- brm(
    formula = ref_thiomon_formulae_separate,
    prior = thiomon_set_priors_informative_separate,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.8,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

### 4.2.1 Checking if the covariates are normally distributed

```{r}
train_data |>
  ggplot(aes(x = hgb)) + xlim(c(0,50)) +
  geom_histogram(binwidth = 0.5, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "MCMC Histogram of Haemoglobin", x = "Parameter Value", y = "Frequency")
```

```{r}
train_data |>
  ggplot(aes(x = hct)) + xlim(c(0,75)) +
  geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "MCMC Histogram of Haemocrit", x = "Parameter Value", y = "Frequency")
```

Both the parameters demonstrate a normal distribution around their respective means. We can approximate with normal distribution for all the parameters.

We don't use flat priors as they are uninformative and we have lots of data to give us good reference values. Also with flat priors, posterior prior checking tools due to input containing infinite or NA values as there is a constant tail. We notice also that the entire population parameters are assigned the same priors. We want to change it. Also notice that we don't have population level Intercept.

### 4.2.2 Resolving problem of prior sensitivity

With our chosen priors, we see a strong prior prior sensitivity. Power-scaling with cumulative Jensen-Shannon distance diagnostic indicates prior-data conflict.

```{r}
thiomon_data_model_separate |>
  powerscale_plot_dens(help_text=FALSE)
```

```{r}
thiomon_data_model_separate |>
  powerscale_sensitivity() |> tt()
```

It turns out our selected priors are hopelessly wrong. Why? The estimated mean of 'b_hgb' is 1.45 [90% range is 0.65 to 2.29]. Similarly for 'b_hct' is -0.51 [90% range is -0.79 to -0.23]. Similarly the intercept for the ratios need to be set wider to accomodate the individual values.

```{r}
ref_thiomon_formulae_separate <- bf(remission ~ 0 + hgb + hct + hbghctrat, family = "bernoulli")

(thiomon_set_priors_informative_separate_2 <- c(
  
  prior(
    normal(0,5),
    ub = 0,
    class = "b"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "hct"
  ),
  
  prior(
    normal(1,2),
    class = "b",
    coef = "hgb"
  )
))

thiomon_data_model_separate <- brm(
    formula = ref_thiomon_formulae_separate,
    prior = thiomon_set_priors_informative_separate_2,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.9,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

And the prior-data conflicts have indeed gone. Lesson learnt and also validates the fundamental concept of Bayesian statistics - one can indeed learn from the posterior of the prior and use that information as a guide to tune the prior.

### 4.2.3 R-hat and Effective Sample Size (ESS)

First checking the model for any discrepancies in R-hat and ESS before proceeding any further.

```{r}
thiomon_data_model_separate
```

R-hat and effective sample sizes look good. At convergence also R-hat value is 1 means the chains have converged well.

```{r}
loo(thiomon_data_model_separate)
```

### 4.2.4 Issue with Pareto K-hat and solution

We get warnings about 1 Pareto k \> 0.7 in PSIS-LOO. We can improve the accuracy be running MCMC for these LOO folds. We use the following function to store the LOO computation results. The model is fit one time again excluding 1 observation.

```{r}
thiomon_data_model_separate <- add_criterion(thiomon_data_model_separate, criterion='loo', reloo=TRUE)
```

```{r}
loo(thiomon_data_model_separate)
```

The Pareto k-estimates indicate there are no observations which have k \> 0.7.

Posterior predictive checks seem fine as well. Density overlay seems to fit well.

### 4.2.5 Posterior predictive checking

```{r}
pp_check(thiomon_data_model_separate, type = 'dens_overlay', ndraws = 200)
```

LOO-PIT check seems fine as well.

```{r}
pp_check(thiomon_data_model_separate, type = 'loo_pit_qq', ndraws = 4000)
```

LOO-PIT ECDF is within the blue recommendation.

```{r}
pp_check(thiomon_data_model_separate, type = 'pit_ecdf', ndraws = 4000)
```

### 4.2.6 Model parameters

The slopes for 'hgb', 'hct' and the 'hgb/hct' ratios can be listed using the following

```{r}
as_draws_df(thiomon_data_model_separate)  |> subset_draws(variable=c('lprior','lp__'), exclude=TRUE) |> summarise_draws() |> tt()
```

The separate model should perform better than the pooled model.

## 4.3. Hierarchical model

Let's try a hierarchical model.

```{r}
ref_thiomon_formulae_hier <- bf(remission ~ 0 + hgb + hct + (1 | hbghctrat), family = "bernoulli")

thiomon_priors_default_priors_hier <- get_prior(ref_thiomon_formulae_hier, data = train_data)

thiomon_set_priors_informative_hier <- c(
  
  prior(
    normal(0.5,2),
    class = "b",
    coef = "hgb"
  ),
  
  prior(
    normal(0,1),
    class = "b",
    coef = "hct"
  ),

  prior(
    normal(5,10),
    class = "sd"
  )
)

thiomon_data_model_hier <- brm(
    formula = ref_thiomon_formulae_hier,
    prior = thiomon_set_priors_informative_hier,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.9,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

### 4.3.1 R-hat and Effective Sample Size (ESS)

R-hat and ESS values look good. Convergence at the end of the run indicates R-hat is 1.0 which means samples are reliable and can be used for inference. Also posterior prior sensitivity is well adjusted.

```{r}
thiomon_data_model_hier |>
  powerscale_sensitivity()
```

### 4.3.2 Issue with Pareto K-hat and solution

```{r}
loo(thiomon_data_model_hier)
```

```{r}
thiomon_data_model_hier <- add_criterion(thiomon_data_model_hier, criterion='loo', reloo=TRUE)
```

### 4.3.3 LOO-PIT check

```{r}
pp_check(thiomon_data_model_hier, type = 'loo_pit_qq', ndraws = 4000)
```

Try to see if a better hierarchical model fits. The 'sd' prior required a bit of tweaking. Keeping it wider results in 1 divergent transaction which we try to avoid by making it narrow but not quite narrow that we get a prior-data conflict. With this balancing there are no problems we spot.

```{r}
ref_thiomon_formulae_hier_1 <- bf(remission ~ 1 + (1 | hbghctrat), family = "bernoulli")

thiomon_priors_default_priors_hier_1 <- get_prior(ref_thiomon_formulae_hier_1, data = train_data)

thiomon_set_priors_informative_hier_1 <- c(
  
  prior(
    normal(0,1.5),
    class = "Intercept",
  ),

  prior(
    normal(1,1.2),
    class = "sd"
  )
)

thiomon_data_model_hier_1 <- brm(
    formula = ref_thiomon_formulae_hier_1,
    prior = thiomon_set_priors_informative_hier_1,
    data = train_data,
    iter = 2000,
    warmup = 1000,
    chains = 4,                           # Number of chains
    control = list(
      adapt_delta = 0.9,                 # Increase acceptance rate target if needed
      max_treedepth = 20                 # Increase the tree depth if needed
    ),
    threads = threading(2), cores = 8, backend = "cmdstanr"
)
```

There is no issue anymore of prior sensitivity.

```{r}
thiomon_data_model_hier_1 |>
  powerscale_sensitivity()
```

## 4.4 Pooled, Separate and Hierarchical model comparision

### 4.4.1 LOO comparing all the 4 models

```{r}
loo_compare(loo(thiomon_data_model_pooled), loo(thiomon_data_model_separate),loo(thiomon_data_model_hier), loo(thiomon_data_model_hier_1)) |>
  as.data.frame() |>
  rownames_to_column("model") |>
  dplyr::select(model, elpd_diff, se_diff) |>
  tt()
```

The best performing model is the baseline hierarchical model followed by the separate, hierarchical model_1 and finally the pooled. The results seem logical due to the following reasons.

### 4.3.2 Model posterior distributions

```{r}
if (!require(patchwork)) {
  install.packages("patchwork")
  library(patchwork)
}

ph <- thiomon_data_model_hier |>
  spread_rvars(b_hgb, b_hct, r_hbghctrat[hbghctrat,]) |>
  mutate(mean_value = b_hgb + b_hct + r_hbghctrat) |>
  ggplot(aes(xdist= inv_logit_scaled(mean_value), y=hbghctrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb/hct Ratio', title='Hierarchical')

ph1 <- thiomon_data_model_hier_1 |>
  spread_rvars(b_Intercept, r_hbghctrat[hbghctrat,]) |>
  mutate(mean_value = b_Intercept + r_hbghctrat) |>
  ggplot(aes(xdist= inv_logit_scaled(mean_value), y=hbghctrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb/hct Ratio', title='Hierarchical')

ps_part <- thiomon_data_model_separate |> 
  spread_rvars(b_hgb, b_hct)

ps_part_fin <- ps_part[rep(1:nrow(ps_part), each = 5), ]

ps <- thiomon_data_model_separate |>
  as_draws_df() |>
  subset_draws(variable = "b_hbghctrat", regex = TRUE) |>
  set_variables(paste0('b_hbghctrat[', 1:5, ']')) |>
  as_draws_rvars() |>
  spread_rvars(b_hbghctrat[hbghctrat]) |> cbind(ps_part_fin) |>
  mutate(mean_value = b_hbghctrat + b_hgb + b_hct) |>
  ggplot(aes(xdist=inv_logit_scaled(mean_value), y=hbghctrat)) +
  stat_halfeye() +
  scale_y_continuous(breaks=1:5) +
  labs(x='Remission factor', y='hgb/hct Ratio', title='Separate')

pp <- thiomon_data_model_pooled |>
  spread_rvars(b_Intercept) |>
  mutate(mean_value = b_Intercept) |>
  ggplot(aes(xdist=inv_logit_scaled(mean_value), y=0)) +
  stat_halfeye() +
  scale_y_continuous(breaks=NULL) +
  labs(x='Remission factor', y='Across all', title='Pooled model')

#(ph / ph1 / ps / pp) * xlim(c(0.0,1.0))
(ph) * xlim(c(0.0,0.2))
```

### 4.3.3 Bayes and LOO R2 checks

## 4.4. Non-Bayesian model

**[I propose to remove this as we have too much content already in my opinion]**

# ~~5. Convergence diagnostics~~

## ~~5.1.~~ $\widehat{R}$

## ~~5.2. Effective sample size~~

## ~~5.3. Divergent transitions~~

## ~~5.4. Conclusion about convergence~~

# ~~6. Posterior predictive checks~~

## ~~6.1. Separate Model~~

## ~~6.2. Hierarchical Model~~

# 7. Predictive Performance Assessment

## 7.1. Model accuracy and Confusion matrix

## 7.2. ROC curves and AUC values

## ~~8. Sensitivity Analysis~~

# 9. Discussion of Issues and Potential Improvements

# 10. Conclusion

# 11. Self-Reflection

## 11.1. Group Learning

## 11.2. Self Learning

# Appendix

## A. Stan code

### A.1. Pooled model

### A.2. Separate model

### A.3. Hierarchical model

## B. Loaded packages

# References
